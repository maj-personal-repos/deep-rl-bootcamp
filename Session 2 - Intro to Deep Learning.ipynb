{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Intro to Deep Learning (emphasis on Supervised Learning)\n",
    "## Deep RL Bootcamp - Florida International University\n",
    "### Miguel Alonso Jr - Associate Professor of Computer Science\n",
    "October 2019\n",
    "<center><img src=\"https://miro.medium.com/max/2000/1*goFgCUHprcroxSLZvROjpg.jpeg\" width=250></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is Deep Learning\n",
    "\n",
    "- Subset of Machine Learning (which itself is a subset of Artificial Intelligence)\n",
    "- Machine Learning - set of algorithms that process data, learn from the data, and then use what is learned to make intelligent decisions.\n",
    "- So what's the difference?\n",
    "    - Deep Learning learns to represent the world (using data) as a nested hierarchy of concepts\n",
    "    - Each \"concept\" is defined in relation to simpler \"concepts\"\n",
    "    - Abstract representations arise from less abstract ones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Intuitive description of DL\n",
    "\n",
    "You can think of DL as a way to represent information as a series of complex abstraction.\n",
    "\n",
    "<center><img src=\"https://miro.medium.com/max/672/1*KYUUg9JC6InYe-VNPMDzAA.png\"></center>\n",
    "\n",
    "(Source: [Towards Datascience](https://towardsdatascience.com/why-deep-learning-is-needed-over-traditional-machine-learning-1b6a99177063))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Interest over time: DL, AI, and ML (2004 - 2019)\n",
    "![Search terms: DL, AI, and ML](https://miro.medium.com/max/1156/1*vd96L7Rv1-yCkqlLOGq7mg.png)\n",
    "(Source: [Medium](https://medium.com/datadriveninvestor/bootstrapping-your-machine-learning-journey-f339e011c6b6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Last five years...\n",
    "![Last 5 years](https://miro.medium.com/max/1154/1*KMm54RZ5hVToYgaXfNELiQ.png)\n",
    "(Source: [Medium](https://medium.com/datadriveninvestor/bootstrapping-your-machine-learning-journey-f339e011c6b6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tradional ML pipeline - Example: Classifier\n",
    "\n",
    "<center><img src=\"https://www.guru99.com/images/tensorflow/083018_0454_MachineLear3.png\"></center>\n",
    "\n",
    "(Source: [Guru99](https://www.guru99.com/machine-learning-vs-deep-learning.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DL pipeline - Example: Classifier\n",
    "\n",
    "<center><img src=\"https://www.guru99.com/images/tensorflow/083018_0454_MachineLear4.png\"></center>\n",
    "\n",
    "(Source: [Guru99](https://www.guru99.com/machine-learning-vs-deep-learning.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Traditional Machine Learning vs DL\n",
    "<center><img src=\"https://www.guru99.com/images/tensorflow/083018_0454_MachineLear5.png\" width=500></center>\n",
    "\n",
    "(Source:[Guru99](https://www.guru99.com/machine-learning-vs-deep-learning.html))\n",
    "\n",
    "- Old dog, new tricks\n",
    "- 2009 - Throw lots of data and lots of compute (i.e. GPUs) towards Artificial Neural Networks and boom!\n",
    "- How to make models better: more compute and more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://miro.medium.com/max/1218/1*mVVglaFUwRz-HkcMlLyvgg.jpeg\" width=800></center>\n",
    "\n",
    "(Source: [Towards Datascience](https://towardsdatascience.com/why-deep-learning-is-needed-over-traditional-machine-learning-1b6a99177063))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"https://miro.medium.com/max/693/1*ZX05x1xYgaVoa4Vn2kKS9g.png\" width=800>\n",
    "</center>\n",
    "\n",
    "(Source: [Towards Datascience](https://towardsdatascience.com/why-deep-learning-is-needed-over-traditional-machine-learning-1b6a99177063S))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Differences between ML and DL\n",
    "\n",
    "|                       | Machine Learning Learning                                                                                  | Deep Learning                                                                                             |\n",
    "|-----------------------|------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------|\n",
    "| Data Dependencies     | Excellent performances on a small/medium dataset                                                           | Excellent performance on a big dataset                                                                    |\n",
    "| Hardware dependencies | Work on a low-end machine.                                                                                 | Requires powerful machine, preferably with GPU: DL performs a significant amount of matrix multiplication |\n",
    "| Feature engineering   | Need to understand the features that represent the data                                                    | No need to understand the best feature that represents the data                                           |\n",
    "| Execution time        | From few minutes to hours                                                                                  | Up to weeks. Neural Network needs to compute a significant number of weights                              |\n",
    "| Interpretability      | Some algorithms are easy to interpret (logistic, decision tree), some are almost impossible (SVM, XGBoost) | Difficult to impossible                                                                                   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Learning Architectures\n",
    "\n",
    "Many types of deep learning architectures: CNNs, RNNs, LSTMS, DBN, DSN, GRU, etc\n",
    "\n",
    "- Convolutional NNs: Image recognition, semantic segmentation, object detection, control policy, video analysis, natural language processing\n",
    "- Recurrent NNs: sequence data\n",
    "        - LSTMs/GRUs: Natural language text compression, handwriting recognition, speech recognition, gesture recognition, image captioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Learning Architectures (cont'd)\n",
    "\n",
    "- Deep Belief N: Image recognition, information retrieval, natural language understanding, failure prediction\n",
    "- Deep Stacking N: Information retrieval, continuous speech recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                    Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a>.<br/>\n",
       "                    Couldn't load entity due to error: Can't connect to network to query entity from API key\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torchviz import make_dot, make_dot_from_trace\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# import wandb\n",
    "# wandb.init(project=\"Deep-RL-BC-Session-2\")\n",
    "\n",
    "GPUS = 1\n",
    "DEVICE = torch.device(\"cuda:0\" if (torch.cuda.is_available() and GPUS > 0) else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_folder = './images'\n",
    "\n",
    "trsfrm = transforms.Compose([transforms.ToTensor(),\n",
    "                             transforms.Normalize((0.5,), (1.0,))])\n",
    "\n",
    "training_set = datasets.MNIST(root=image_folder, train=True, transform=trsfrm, download=True)\n",
    "testing_set = datasets.MNIST(root=image_folder, train=False, transform=trsfrm, download=True)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(dataset=training_set,\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=True)\n",
    "\n",
    "testing_loader = torch.utils.data.DataLoader(dataset=testing_set,\n",
    "                                             batch_size=batch_size,\n",
    "                                             shuffle=True)\n",
    "dataiter = iter(training_loader)\n",
    "images, labels = dataiter.next()\n",
    "images.size()\n",
    "\n",
    "# wandb.config.batch_size = batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f123cb500f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's take a look at the training data\n",
    "image_grid = torchvision.utils.make_grid(images,normalize=True)\n",
    "plt.imshow(np.transpose(image_grid.numpy(),(1,2,0)), interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SoftmaxRegression, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 512)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)  # this flattens the image into a vector\n",
    "        x = self.fc1(x)\n",
    "        return F.log_softmax(x, 1)\n",
    "        \n",
    "    \n",
    "    def name(self):\n",
    "        return \"SoftmaxRegression\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to instatiate the network and define the optimizer and loss\n",
    "smr = SoftmaxRegression()\n",
    "smr.to(device=DEVICE)\n",
    "optimizer = optim.SGD(smr.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"154pt\" height=\"226pt\"\n",
       " viewBox=\"0.00 0.00 153.50 226.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 222)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-222 149.5,-222 149.5,4 -4,4\"/>\n",
       "<!-- 140080479815104 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>140080479815104</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"130.5,-21 5.5,-21 5.5,0 130.5,0 130.5,-21\"/>\n",
       "<text text-anchor=\"middle\" x=\"68\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">LogSoftmaxBackward</text>\n",
       "</g>\n",
       "<!-- 140080479816224 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>140080479816224</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"120,-78 16,-78 16,-57 120,-57 120,-78\"/>\n",
       "<text text-anchor=\"middle\" x=\"68\" y=\"-64.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddmmBackward</text>\n",
       "</g>\n",
       "<!-- 140080479816224&#45;&gt;140080479815104 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>140080479816224&#45;&gt;140080479815104</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M68,-56.7787C68,-49.6134 68,-39.9517 68,-31.3097\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"71.5001,-31.1732 68,-21.1732 64.5001,-31.1732 71.5001,-31.1732\"/>\n",
       "</g>\n",
       "<!-- 140080479815496 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>140080479815496</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"54,-148 0,-148 0,-114 54,-114 54,-148\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-134.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc1.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-121.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (512)</text>\n",
       "</g>\n",
       "<!-- 140080479815496&#45;&gt;140080479816224 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>140080479815496&#45;&gt;140080479816224</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M37.9872,-113.9832C43.3524,-105.6737 49.8346,-95.6342 55.3998,-87.015\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"58.4985,-88.6682 60.9824,-78.3687 52.6177,-84.8712 58.4985,-88.6682\"/>\n",
       "</g>\n",
       "<!-- 140080479817456 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>140080479817456</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"145.5,-141.5 72.5,-141.5 72.5,-120.5 145.5,-120.5 145.5,-141.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"109\" y=\"-127.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">TBackward</text>\n",
       "</g>\n",
       "<!-- 140080479817456&#45;&gt;140080479816224 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>140080479817456&#45;&gt;140080479816224</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M102.0449,-120.2281C96.1191,-111.0503 87.4601,-97.6394 80.3579,-86.6396\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"83.2171,-84.6153 74.8524,-78.1128 77.3363,-88.4124 83.2171,-84.6153\"/>\n",
       "</g>\n",
       "<!-- 140080479817232 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>140080479817232</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"143.5,-218 74.5,-218 74.5,-184 143.5,-184 143.5,-218\"/>\n",
       "<text text-anchor=\"middle\" x=\"109\" y=\"-204.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc1.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"109\" y=\"-191.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (512, 784)</text>\n",
       "</g>\n",
       "<!-- 140080479817232&#45;&gt;140080479817456 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>140080479817232&#45;&gt;140080479817456</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M109,-183.6966C109,-174.0634 109,-162.003 109,-151.8518\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"112.5001,-151.7912 109,-141.7913 105.5001,-151.7913 112.5001,-151.7912\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f67073c2da0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's visualize the network architecture\n",
    "make_dot(smr(images.to(device=DEVICE)), params=dict(smr.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> epoch: 0, batch index: 100, train loss: 1.747986, acc: 0.446\n",
      "==>>> epoch: 0, batch index: 200, train loss: 1.216145, acc: 0.597\n",
      "==>>> epoch: 0, batch index: 300, train loss: 1.007843, acc: 0.663\n",
      "==>>> epoch: 0, batch index: 400, train loss: 0.885285, acc: 0.702\n",
      "==>>> epoch: 0, batch index: 500, train loss: 0.790445, acc: 0.727\n",
      "==>>> epoch: 0, batch index: 600, train loss: 0.713073, acc: 0.747\n",
      "==>>> epoch: 0, batch index: 700, train loss: 0.699475, acc: 0.761\n",
      "==>>> epoch: 0, batch index: 800, train loss: 0.649839, acc: 0.772\n",
      "==>>> epoch: 0, batch index: 900, train loss: 0.599870, acc: 0.781\n",
      "==>>> epoch: 0, batch index: 938, train loss: 0.643617, acc: 0.784\n",
      "==>>> epoch: 0, batch index: 100, test loss: 0.595461, acc: 0.867\n",
      "==>>> epoch: 0, batch index: 157, test loss: 0.606379, acc: 0.864\n",
      "==>>> epoch: 1, batch index: 100, train loss: 0.590692, acc: 0.864\n",
      "==>>> epoch: 1, batch index: 200, train loss: 0.580722, acc: 0.862\n",
      "==>>> epoch: 1, batch index: 300, train loss: 0.551967, acc: 0.862\n",
      "==>>> epoch: 1, batch index: 400, train loss: 0.536280, acc: 0.864\n",
      "==>>> epoch: 1, batch index: 500, train loss: 0.521228, acc: 0.864\n",
      "==>>> epoch: 1, batch index: 600, train loss: 0.545312, acc: 0.864\n",
      "==>>> epoch: 1, batch index: 700, train loss: 0.476469, acc: 0.865\n",
      "==>>> epoch: 1, batch index: 800, train loss: 0.517032, acc: 0.867\n",
      "==>>> epoch: 1, batch index: 900, train loss: 0.460950, acc: 0.868\n",
      "==>>> epoch: 1, batch index: 938, train loss: 0.495701, acc: 0.868\n",
      "==>>> epoch: 1, batch index: 100, test loss: 0.457553, acc: 0.877\n",
      "==>>> epoch: 1, batch index: 157, test loss: 0.508145, acc: 0.882\n",
      "==>>> epoch: 2, batch index: 100, train loss: 0.460176, acc: 0.873\n",
      "==>>> epoch: 2, batch index: 200, train loss: 0.451516, acc: 0.878\n",
      "==>>> epoch: 2, batch index: 300, train loss: 0.498795, acc: 0.878\n",
      "==>>> epoch: 2, batch index: 400, train loss: 0.486436, acc: 0.878\n",
      "==>>> epoch: 2, batch index: 500, train loss: 0.448104, acc: 0.879\n",
      "==>>> epoch: 2, batch index: 600, train loss: 0.444448, acc: 0.879\n",
      "==>>> epoch: 2, batch index: 700, train loss: 0.439175, acc: 0.879\n",
      "==>>> epoch: 2, batch index: 800, train loss: 0.494286, acc: 0.879\n",
      "==>>> epoch: 2, batch index: 900, train loss: 0.443327, acc: 0.880\n",
      "==>>> epoch: 2, batch index: 938, train loss: 0.496996, acc: 0.880\n",
      "==>>> epoch: 2, batch index: 100, test loss: 0.436559, acc: 0.889\n",
      "==>>> epoch: 2, batch index: 157, test loss: 0.403567, acc: 0.891\n",
      "==>>> epoch: 3, batch index: 100, train loss: 0.458935, acc: 0.886\n",
      "==>>> epoch: 3, batch index: 200, train loss: 0.433265, acc: 0.884\n",
      "==>>> epoch: 3, batch index: 300, train loss: 0.435166, acc: 0.888\n",
      "==>>> epoch: 3, batch index: 400, train loss: 0.409113, acc: 0.888\n",
      "==>>> epoch: 3, batch index: 500, train loss: 0.374942, acc: 0.888\n",
      "==>>> epoch: 3, batch index: 600, train loss: 0.430657, acc: 0.887\n",
      "==>>> epoch: 3, batch index: 700, train loss: 0.440619, acc: 0.887\n",
      "==>>> epoch: 3, batch index: 800, train loss: 0.436424, acc: 0.887\n",
      "==>>> epoch: 3, batch index: 900, train loss: 0.476265, acc: 0.887\n",
      "==>>> epoch: 3, batch index: 938, train loss: 0.411624, acc: 0.886\n",
      "==>>> epoch: 3, batch index: 100, test loss: 0.417710, acc: 0.900\n",
      "==>>> epoch: 3, batch index: 157, test loss: 0.404359, acc: 0.896\n",
      "==>>> epoch: 4, batch index: 100, train loss: 0.399121, acc: 0.890\n",
      "==>>> epoch: 4, batch index: 200, train loss: 0.390647, acc: 0.890\n",
      "==>>> epoch: 4, batch index: 300, train loss: 0.399656, acc: 0.889\n",
      "==>>> epoch: 4, batch index: 400, train loss: 0.432951, acc: 0.889\n",
      "==>>> epoch: 4, batch index: 500, train loss: 0.408895, acc: 0.890\n",
      "==>>> epoch: 4, batch index: 600, train loss: 0.396068, acc: 0.890\n",
      "==>>> epoch: 4, batch index: 700, train loss: 0.390634, acc: 0.890\n",
      "==>>> epoch: 4, batch index: 800, train loss: 0.398703, acc: 0.890\n",
      "==>>> epoch: 4, batch index: 900, train loss: 0.384886, acc: 0.891\n",
      "==>>> epoch: 4, batch index: 938, train loss: 0.399167, acc: 0.891\n",
      "==>>> epoch: 4, batch index: 100, test loss: 0.365293, acc: 0.900\n",
      "==>>> epoch: 4, batch index: 157, test loss: 0.361258, acc: 0.899\n",
      "==>>> epoch: 5, batch index: 100, train loss: 0.374121, acc: 0.893\n",
      "==>>> epoch: 5, batch index: 200, train loss: 0.370611, acc: 0.893\n",
      "==>>> epoch: 5, batch index: 300, train loss: 0.389786, acc: 0.893\n",
      "==>>> epoch: 5, batch index: 400, train loss: 0.420367, acc: 0.894\n",
      "==>>> epoch: 5, batch index: 500, train loss: 0.343112, acc: 0.894\n",
      "==>>> epoch: 5, batch index: 600, train loss: 0.369545, acc: 0.894\n",
      "==>>> epoch: 5, batch index: 700, train loss: 0.357709, acc: 0.894\n",
      "==>>> epoch: 5, batch index: 800, train loss: 0.383865, acc: 0.894\n",
      "==>>> epoch: 5, batch index: 900, train loss: 0.371341, acc: 0.894\n",
      "==>>> epoch: 5, batch index: 938, train loss: 0.353351, acc: 0.894\n",
      "==>>> epoch: 5, batch index: 100, test loss: 0.340170, acc: 0.902\n",
      "==>>> epoch: 5, batch index: 157, test loss: 0.387386, acc: 0.902\n",
      "==>>> epoch: 6, batch index: 100, train loss: 0.374605, acc: 0.901\n",
      "==>>> epoch: 6, batch index: 200, train loss: 0.381946, acc: 0.901\n",
      "==>>> epoch: 6, batch index: 300, train loss: 0.399628, acc: 0.900\n",
      "==>>> epoch: 6, batch index: 400, train loss: 0.372505, acc: 0.901\n",
      "==>>> epoch: 6, batch index: 500, train loss: 0.391661, acc: 0.899\n",
      "==>>> epoch: 6, batch index: 600, train loss: 0.381205, acc: 0.897\n",
      "==>>> epoch: 6, batch index: 700, train loss: 0.403672, acc: 0.897\n",
      "==>>> epoch: 6, batch index: 800, train loss: 0.363582, acc: 0.897\n",
      "==>>> epoch: 6, batch index: 900, train loss: 0.376588, acc: 0.896\n",
      "==>>> epoch: 6, batch index: 938, train loss: 0.387233, acc: 0.897\n",
      "==>>> epoch: 6, batch index: 100, test loss: 0.379559, acc: 0.902\n",
      "==>>> epoch: 6, batch index: 157, test loss: 0.312593, acc: 0.904\n",
      "==>>> epoch: 7, batch index: 100, train loss: 0.371766, acc: 0.898\n",
      "==>>> epoch: 7, batch index: 200, train loss: 0.363338, acc: 0.902\n",
      "==>>> epoch: 7, batch index: 300, train loss: 0.393323, acc: 0.899\n",
      "==>>> epoch: 7, batch index: 400, train loss: 0.376454, acc: 0.899\n",
      "==>>> epoch: 7, batch index: 500, train loss: 0.338443, acc: 0.897\n",
      "==>>> epoch: 7, batch index: 600, train loss: 0.389326, acc: 0.898\n",
      "==>>> epoch: 7, batch index: 700, train loss: 0.374219, acc: 0.898\n",
      "==>>> epoch: 7, batch index: 800, train loss: 0.378462, acc: 0.899\n",
      "==>>> epoch: 7, batch index: 900, train loss: 0.390061, acc: 0.899\n",
      "==>>> epoch: 7, batch index: 938, train loss: 0.378413, acc: 0.899\n",
      "==>>> epoch: 7, batch index: 100, test loss: 0.314938, acc: 0.906\n",
      "==>>> epoch: 7, batch index: 157, test loss: 0.363154, acc: 0.905\n",
      "==>>> epoch: 8, batch index: 100, train loss: 0.366478, acc: 0.900\n",
      "==>>> epoch: 8, batch index: 200, train loss: 0.375421, acc: 0.899\n",
      "==>>> epoch: 8, batch index: 300, train loss: 0.349918, acc: 0.899\n",
      "==>>> epoch: 8, batch index: 400, train loss: 0.339573, acc: 0.899\n",
      "==>>> epoch: 8, batch index: 500, train loss: 0.335123, acc: 0.900\n",
      "==>>> epoch: 8, batch index: 600, train loss: 0.372012, acc: 0.900\n",
      "==>>> epoch: 8, batch index: 700, train loss: 0.355655, acc: 0.900\n",
      "==>>> epoch: 8, batch index: 800, train loss: 0.352942, acc: 0.900\n",
      "==>>> epoch: 8, batch index: 900, train loss: 0.311433, acc: 0.900\n",
      "==>>> epoch: 8, batch index: 938, train loss: 0.325879, acc: 0.901\n",
      "==>>> epoch: 8, batch index: 100, test loss: 0.318594, acc: 0.905\n",
      "==>>> epoch: 8, batch index: 157, test loss: 0.329458, acc: 0.907\n",
      "==>>> epoch: 9, batch index: 100, train loss: 0.372838, acc: 0.901\n",
      "==>>> epoch: 9, batch index: 200, train loss: 0.330451, acc: 0.904\n",
      "==>>> epoch: 9, batch index: 300, train loss: 0.352665, acc: 0.904\n",
      "==>>> epoch: 9, batch index: 400, train loss: 0.322073, acc: 0.902\n",
      "==>>> epoch: 9, batch index: 500, train loss: 0.361614, acc: 0.901\n",
      "==>>> epoch: 9, batch index: 600, train loss: 0.347771, acc: 0.902\n",
      "==>>> epoch: 9, batch index: 700, train loss: 0.377983, acc: 0.902\n",
      "==>>> epoch: 9, batch index: 800, train loss: 0.369460, acc: 0.902\n",
      "==>>> epoch: 9, batch index: 900, train loss: 0.364250, acc: 0.902\n",
      "==>>> epoch: 9, batch index: 938, train loss: 0.322222, acc: 0.902\n",
      "==>>> epoch: 9, batch index: 100, test loss: 0.367157, acc: 0.908\n",
      "==>>> epoch: 9, batch index: 157, test loss: 0.322605, acc: 0.908\n",
      "==>>> epoch: 10, batch index: 100, train loss: 0.375814, acc: 0.899\n",
      "==>>> epoch: 10, batch index: 200, train loss: 0.337636, acc: 0.900\n",
      "==>>> epoch: 10, batch index: 300, train loss: 0.322849, acc: 0.902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> epoch: 10, batch index: 400, train loss: 0.361566, acc: 0.903\n",
      "==>>> epoch: 10, batch index: 500, train loss: 0.344880, acc: 0.903\n",
      "==>>> epoch: 10, batch index: 600, train loss: 0.309416, acc: 0.903\n",
      "==>>> epoch: 10, batch index: 700, train loss: 0.365258, acc: 0.903\n",
      "==>>> epoch: 10, batch index: 800, train loss: 0.320906, acc: 0.903\n",
      "==>>> epoch: 10, batch index: 900, train loss: 0.341671, acc: 0.903\n",
      "==>>> epoch: 10, batch index: 938, train loss: 0.317066, acc: 0.903\n",
      "==>>> epoch: 10, batch index: 100, test loss: 0.356324, acc: 0.909\n",
      "==>>> epoch: 10, batch index: 157, test loss: 0.303693, acc: 0.909\n",
      "==>>> epoch: 11, batch index: 100, train loss: 0.376656, acc: 0.910\n",
      "==>>> epoch: 11, batch index: 200, train loss: 0.363194, acc: 0.905\n",
      "==>>> epoch: 11, batch index: 300, train loss: 0.307322, acc: 0.907\n",
      "==>>> epoch: 11, batch index: 400, train loss: 0.352828, acc: 0.906\n",
      "==>>> epoch: 11, batch index: 500, train loss: 0.389332, acc: 0.906\n",
      "==>>> epoch: 11, batch index: 600, train loss: 0.406762, acc: 0.905\n",
      "==>>> epoch: 11, batch index: 700, train loss: 0.335255, acc: 0.904\n",
      "==>>> epoch: 11, batch index: 800, train loss: 0.309615, acc: 0.905\n",
      "==>>> epoch: 11, batch index: 900, train loss: 0.311681, acc: 0.905\n",
      "==>>> epoch: 11, batch index: 938, train loss: 0.375541, acc: 0.904\n",
      "==>>> epoch: 11, batch index: 100, test loss: 0.289426, acc: 0.909\n",
      "==>>> epoch: 11, batch index: 157, test loss: 0.331304, acc: 0.910\n",
      "==>>> epoch: 12, batch index: 100, train loss: 0.374296, acc: 0.905\n",
      "==>>> epoch: 12, batch index: 200, train loss: 0.370991, acc: 0.902\n",
      "==>>> epoch: 12, batch index: 300, train loss: 0.371046, acc: 0.903\n",
      "==>>> epoch: 12, batch index: 400, train loss: 0.388234, acc: 0.903\n",
      "==>>> epoch: 12, batch index: 500, train loss: 0.326608, acc: 0.904\n",
      "==>>> epoch: 12, batch index: 600, train loss: 0.326920, acc: 0.904\n",
      "==>>> epoch: 12, batch index: 700, train loss: 0.312024, acc: 0.905\n",
      "==>>> epoch: 12, batch index: 800, train loss: 0.340837, acc: 0.906\n",
      "==>>> epoch: 12, batch index: 900, train loss: 0.361250, acc: 0.906\n",
      "==>>> epoch: 12, batch index: 938, train loss: 0.304186, acc: 0.906\n",
      "==>>> epoch: 12, batch index: 100, test loss: 0.285441, acc: 0.914\n",
      "==>>> epoch: 12, batch index: 157, test loss: 0.330404, acc: 0.911\n",
      "==>>> epoch: 13, batch index: 100, train loss: 0.346639, acc: 0.909\n",
      "==>>> epoch: 13, batch index: 200, train loss: 0.329448, acc: 0.910\n",
      "==>>> epoch: 13, batch index: 300, train loss: 0.361757, acc: 0.908\n",
      "==>>> epoch: 13, batch index: 400, train loss: 0.351202, acc: 0.908\n",
      "==>>> epoch: 13, batch index: 500, train loss: 0.356208, acc: 0.907\n",
      "==>>> epoch: 13, batch index: 600, train loss: 0.308844, acc: 0.906\n",
      "==>>> epoch: 13, batch index: 700, train loss: 0.324303, acc: 0.907\n",
      "==>>> epoch: 13, batch index: 800, train loss: 0.357032, acc: 0.907\n",
      "==>>> epoch: 13, batch index: 900, train loss: 0.340097, acc: 0.906\n",
      "==>>> epoch: 13, batch index: 938, train loss: 0.325145, acc: 0.906\n",
      "==>>> epoch: 13, batch index: 100, test loss: 0.297602, acc: 0.914\n",
      "==>>> epoch: 13, batch index: 157, test loss: 0.311220, acc: 0.912\n",
      "==>>> epoch: 14, batch index: 100, train loss: 0.394071, acc: 0.909\n",
      "==>>> epoch: 14, batch index: 200, train loss: 0.302696, acc: 0.908\n",
      "==>>> epoch: 14, batch index: 300, train loss: 0.311026, acc: 0.906\n",
      "==>>> epoch: 14, batch index: 400, train loss: 0.331296, acc: 0.906\n",
      "==>>> epoch: 14, batch index: 500, train loss: 0.357056, acc: 0.907\n",
      "==>>> epoch: 14, batch index: 600, train loss: 0.319752, acc: 0.908\n",
      "==>>> epoch: 14, batch index: 700, train loss: 0.320189, acc: 0.907\n",
      "==>>> epoch: 14, batch index: 800, train loss: 0.325121, acc: 0.907\n",
      "==>>> epoch: 14, batch index: 900, train loss: 0.322471, acc: 0.908\n",
      "==>>> epoch: 14, batch index: 938, train loss: 0.322273, acc: 0.908\n",
      "==>>> epoch: 14, batch index: 100, test loss: 0.328074, acc: 0.911\n",
      "==>>> epoch: 14, batch index: 157, test loss: 0.312068, acc: 0.911\n",
      "==>>> epoch: 15, batch index: 100, train loss: 0.295819, acc: 0.914\n",
      "==>>> epoch: 15, batch index: 200, train loss: 0.312122, acc: 0.911\n",
      "==>>> epoch: 15, batch index: 300, train loss: 0.317479, acc: 0.910\n",
      "==>>> epoch: 15, batch index: 400, train loss: 0.319718, acc: 0.909\n",
      "==>>> epoch: 15, batch index: 500, train loss: 0.326736, acc: 0.908\n",
      "==>>> epoch: 15, batch index: 600, train loss: 0.322412, acc: 0.908\n",
      "==>>> epoch: 15, batch index: 700, train loss: 0.368170, acc: 0.907\n",
      "==>>> epoch: 15, batch index: 800, train loss: 0.317251, acc: 0.908\n",
      "==>>> epoch: 15, batch index: 900, train loss: 0.357644, acc: 0.908\n",
      "==>>> epoch: 15, batch index: 938, train loss: 0.281889, acc: 0.908\n",
      "==>>> epoch: 15, batch index: 100, test loss: 0.290589, acc: 0.914\n",
      "==>>> epoch: 15, batch index: 157, test loss: 0.309317, acc: 0.913\n",
      "==>>> epoch: 16, batch index: 100, train loss: 0.303237, acc: 0.906\n",
      "==>>> epoch: 16, batch index: 200, train loss: 0.317822, acc: 0.904\n",
      "==>>> epoch: 16, batch index: 300, train loss: 0.311784, acc: 0.907\n",
      "==>>> epoch: 16, batch index: 400, train loss: 0.285227, acc: 0.908\n",
      "==>>> epoch: 16, batch index: 500, train loss: 0.288222, acc: 0.909\n",
      "==>>> epoch: 16, batch index: 600, train loss: 0.337454, acc: 0.909\n",
      "==>>> epoch: 16, batch index: 700, train loss: 0.364341, acc: 0.909\n",
      "==>>> epoch: 16, batch index: 800, train loss: 0.322335, acc: 0.908\n",
      "==>>> epoch: 16, batch index: 900, train loss: 0.324116, acc: 0.909\n",
      "==>>> epoch: 16, batch index: 938, train loss: 0.289478, acc: 0.909\n",
      "==>>> epoch: 16, batch index: 100, test loss: 0.339773, acc: 0.909\n",
      "==>>> epoch: 16, batch index: 157, test loss: 0.280712, acc: 0.913\n",
      "==>>> epoch: 17, batch index: 100, train loss: 0.330381, acc: 0.904\n",
      "==>>> epoch: 17, batch index: 200, train loss: 0.310627, acc: 0.907\n",
      "==>>> epoch: 17, batch index: 300, train loss: 0.317795, acc: 0.909\n",
      "==>>> epoch: 17, batch index: 400, train loss: 0.349214, acc: 0.907\n",
      "==>>> epoch: 17, batch index: 500, train loss: 0.308213, acc: 0.908\n",
      "==>>> epoch: 17, batch index: 600, train loss: 0.350199, acc: 0.908\n",
      "==>>> epoch: 17, batch index: 700, train loss: 0.276301, acc: 0.909\n",
      "==>>> epoch: 17, batch index: 800, train loss: 0.357706, acc: 0.909\n",
      "==>>> epoch: 17, batch index: 900, train loss: 0.339416, acc: 0.909\n",
      "==>>> epoch: 17, batch index: 938, train loss: 0.251314, acc: 0.909\n",
      "==>>> epoch: 17, batch index: 100, test loss: 0.305538, acc: 0.912\n",
      "==>>> epoch: 17, batch index: 157, test loss: 0.310099, acc: 0.913\n",
      "==>>> epoch: 18, batch index: 100, train loss: 0.318085, acc: 0.912\n",
      "==>>> epoch: 18, batch index: 200, train loss: 0.322979, acc: 0.912\n",
      "==>>> epoch: 18, batch index: 300, train loss: 0.313685, acc: 0.912\n",
      "==>>> epoch: 18, batch index: 400, train loss: 0.343696, acc: 0.910\n",
      "==>>> epoch: 18, batch index: 500, train loss: 0.336509, acc: 0.910\n",
      "==>>> epoch: 18, batch index: 600, train loss: 0.276065, acc: 0.910\n",
      "==>>> epoch: 18, batch index: 700, train loss: 0.280820, acc: 0.910\n",
      "==>>> epoch: 18, batch index: 800, train loss: 0.288506, acc: 0.911\n",
      "==>>> epoch: 18, batch index: 900, train loss: 0.342563, acc: 0.911\n",
      "==>>> epoch: 18, batch index: 938, train loss: 0.324368, acc: 0.910\n",
      "==>>> epoch: 18, batch index: 100, test loss: 0.335495, acc: 0.915\n",
      "==>>> epoch: 18, batch index: 157, test loss: 0.280048, acc: 0.914\n",
      "==>>> epoch: 19, batch index: 100, train loss: 0.332924, acc: 0.910\n",
      "==>>> epoch: 19, batch index: 200, train loss: 0.370593, acc: 0.909\n",
      "==>>> epoch: 19, batch index: 300, train loss: 0.309153, acc: 0.910\n",
      "==>>> epoch: 19, batch index: 400, train loss: 0.314861, acc: 0.910\n",
      "==>>> epoch: 19, batch index: 500, train loss: 0.308691, acc: 0.910\n",
      "==>>> epoch: 19, batch index: 600, train loss: 0.284328, acc: 0.911\n",
      "==>>> epoch: 19, batch index: 700, train loss: 0.318386, acc: 0.911\n",
      "==>>> epoch: 19, batch index: 800, train loss: 0.292536, acc: 0.911\n",
      "==>>> epoch: 19, batch index: 900, train loss: 0.286101, acc: 0.911\n",
      "==>>> epoch: 19, batch index: 938, train loss: 0.356850, acc: 0.911\n",
      "==>>> epoch: 19, batch index: 100, test loss: 0.306629, acc: 0.914\n",
      "==>>> epoch: 19, batch index: 157, test loss: 0.330886, acc: 0.914\n"
     ]
    }
   ],
   "source": [
    "# now let's train!\n",
    "\n",
    "for epoch in range(20):\n",
    "    correct_cnt, ave_loss = 0, 0\n",
    "    total_cnt = 0\n",
    "    smr.train()\n",
    "    for batch_idx, (x, target) in enumerate(training_loader):\n",
    "        optimizer.zero_grad()\n",
    "        out = smr(x.to(device=DEVICE))\n",
    "        loss = criterion(out, target.to(device=DEVICE))\n",
    "        _, pred_label = torch.max(out.data, 1)\n",
    "        total_cnt += x.shape[0]\n",
    "        correct_cnt += (pred_label == target.to(device=DEVICE)).sum().item()\n",
    "        ave_loss = ave_loss * 0.9 + loss.item() * 0.1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (batch_idx + 1) % 100 == 0 or (batch_idx + 1) == len(training_loader):\n",
    "            print('==>>> epoch: {}, batch index: {}, train loss: {:.6f}, acc: {:.3f}'.format(\n",
    "                epoch, batch_idx + 1, ave_loss, correct_cnt * 1.0 / total_cnt))\n",
    "\n",
    "    # testing\n",
    "    correct_cnt, ave_loss = 0, 0\n",
    "    total_cnt = 0\n",
    "    smr.eval()\n",
    "    for batch_idx, (x, target) in enumerate(testing_loader):\n",
    "        out = smr(x.to(device=DEVICE))\n",
    "        loss = criterion(out, target.to(device=DEVICE))\n",
    "        _, pred_label = torch.max(out.data, 1)\n",
    "        total_cnt += x.shape[0]\n",
    "        #         print(target.data)\n",
    "        correct_cnt += (pred_label == target.to(device=DEVICE)).sum().item()\n",
    "        # smooth average\n",
    "        ave_loss = ave_loss * 0.9 + loss.item() * 0.1\n",
    "\n",
    "        if (batch_idx + 1) % 100 == 0 or (batch_idx + 1) == len(testing_loader):\n",
    "            print('==>>> epoch: {}, batch index: {}, test loss: {:.6f}, acc: {:.3f}'.format(\n",
    "                epoch, batch_idx + 1, ave_loss, correct_cnt * 1.0 / total_cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)  # this flattens the image into a vector\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, 1)\n",
    "\n",
    "    def name(self):\n",
    "        return \"MultiLayerPerceptron\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to instatiate the network and define the optimizer and loss\n",
    "mlp = MultiLayerPerceptron()\n",
    "mlp.to(device=DEVICE)\n",
    "optimizer = optim.SGD(mlp.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"268pt\" height=\"506pt\"\n",
       " viewBox=\"0.00 0.00 267.50 506.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 502)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-502 263.5,-502 263.5,4 -4,4\"/>\n",
       "<!-- 140080481694776 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>140080481694776</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"182.5,-21 57.5,-21 57.5,0 182.5,0 182.5,-21\"/>\n",
       "<text text-anchor=\"middle\" x=\"120\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">LogSoftmaxBackward</text>\n",
       "</g>\n",
       "<!-- 140080481694888 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>140080481694888</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"172,-78 68,-78 68,-57 172,-57 172,-78\"/>\n",
       "<text text-anchor=\"middle\" x=\"120\" y=\"-64.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddmmBackward</text>\n",
       "</g>\n",
       "<!-- 140080481694888&#45;&gt;140080481694776 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>140080481694888&#45;&gt;140080481694776</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M120,-56.7787C120,-49.6134 120,-39.9517 120,-31.3097\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"123.5001,-31.1732 120,-21.1732 116.5001,-31.1732 123.5001,-31.1732\"/>\n",
       "</g>\n",
       "<!-- 140080481695672 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>140080481695672</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"55,-148 1,-148 1,-114 55,-114 55,-148\"/>\n",
       "<text text-anchor=\"middle\" x=\"28\" y=\"-134.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc3.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"28\" y=\"-121.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (10)</text>\n",
       "</g>\n",
       "<!-- 140080481695672&#45;&gt;140080481694888 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>140080481695672&#45;&gt;140080481694888</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M52.6543,-113.9832C66.1894,-104.641 82.8926,-93.1122 96.278,-83.8734\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"98.2865,-86.7398 104.5283,-78.1788 94.3102,-80.9788 98.2865,-86.7398\"/>\n",
       "</g>\n",
       "<!-- 140080481694664 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>140080481694664</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"167,-141.5 73,-141.5 73,-120.5 167,-120.5 167,-141.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"120\" y=\"-127.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ReluBackward0</text>\n",
       "</g>\n",
       "<!-- 140080481694664&#45;&gt;140080481694888 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>140080481694664&#45;&gt;140080481694888</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M120,-120.2281C120,-111.5091 120,-98.9699 120,-88.3068\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"123.5001,-88.1128 120,-78.1128 116.5001,-88.1129 123.5001,-88.1128\"/>\n",
       "</g>\n",
       "<!-- 140080481693936 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>140080481693936</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"171,-211.5 67,-211.5 67,-190.5 171,-190.5 171,-211.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"119\" y=\"-197.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddmmBackward</text>\n",
       "</g>\n",
       "<!-- 140080481693936&#45;&gt;140080481694664 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>140080481693936&#45;&gt;140080481694664</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M119.1519,-190.3685C119.2972,-180.1925 119.5206,-164.5606 119.7016,-151.8912\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"123.2034,-151.7806 119.8467,-141.7315 116.2041,-151.6805 123.2034,-151.7806\"/>\n",
       "</g>\n",
       "<!-- 140080481695504 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>140080481695504</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"54,-288 0,-288 0,-254 54,-254 54,-288\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-274.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc2.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-261.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (256)</text>\n",
       "</g>\n",
       "<!-- 140080481695504&#45;&gt;140080481693936 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>140080481695504&#45;&gt;140080481693936</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M49.7416,-253.6966C64.1068,-242.7666 82.5787,-228.7119 96.8325,-217.8666\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"99.2491,-220.4258 105.0881,-211.5852 95.0104,-214.855 99.2491,-220.4258\"/>\n",
       "</g>\n",
       "<!-- 140080481695112 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>140080481695112</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"166,-281.5 72,-281.5 72,-260.5 166,-260.5 166,-281.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"119\" y=\"-267.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ReluBackward0</text>\n",
       "</g>\n",
       "<!-- 140080481695112&#45;&gt;140080481693936 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>140080481695112&#45;&gt;140080481693936</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M119,-260.3685C119,-250.1925 119,-234.5606 119,-221.8912\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"122.5001,-221.7315 119,-211.7315 115.5001,-221.7316 122.5001,-221.7315\"/>\n",
       "</g>\n",
       "<!-- 140080481694832 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>140080481694832</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"170,-351.5 66,-351.5 66,-330.5 170,-330.5 170,-351.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"118\" y=\"-337.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddmmBackward</text>\n",
       "</g>\n",
       "<!-- 140080481694832&#45;&gt;140080481695112 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>140080481694832&#45;&gt;140080481695112</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M118.1519,-330.3685C118.2972,-320.1925 118.5206,-304.5606 118.7016,-291.8912\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"122.2034,-291.7806 118.8467,-281.7315 115.2041,-291.6805 122.2034,-291.7806\"/>\n",
       "</g>\n",
       "<!-- 140080481694048 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>140080481694048</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"104,-428 50,-428 50,-394 104,-394 104,-428\"/>\n",
       "<text text-anchor=\"middle\" x=\"77\" y=\"-414.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc1.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"77\" y=\"-401.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (512)</text>\n",
       "</g>\n",
       "<!-- 140080481694048&#45;&gt;140080481694832 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>140080481694048&#45;&gt;140080481694832</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M87.1348,-393.6966C93.0172,-383.6535 100.4448,-370.9722 106.5395,-360.5667\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"109.6454,-362.189 111.6794,-351.7913 103.6052,-358.6512 109.6454,-362.189\"/>\n",
       "</g>\n",
       "<!-- 140080481694608 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>140080481694608</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"195.5,-421.5 122.5,-421.5 122.5,-400.5 195.5,-400.5 195.5,-421.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"159\" y=\"-407.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">TBackward</text>\n",
       "</g>\n",
       "<!-- 140080481694608&#45;&gt;140080481694832 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>140080481694608&#45;&gt;140080481694832</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M152.773,-400.3685C146.5719,-389.7814 136.9119,-373.2886 129.3456,-360.3705\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"132.3598,-358.5915 124.2856,-351.7315 126.3196,-362.1293 132.3598,-358.5915\"/>\n",
       "</g>\n",
       "<!-- 140080481695168 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>140080481695168</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"193.5,-498 124.5,-498 124.5,-464 193.5,-464 193.5,-498\"/>\n",
       "<text text-anchor=\"middle\" x=\"159\" y=\"-484.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc1.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"159\" y=\"-471.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (512, 784)</text>\n",
       "</g>\n",
       "<!-- 140080481695168&#45;&gt;140080481694608 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>140080481695168&#45;&gt;140080481694608</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M159,-463.6966C159,-454.0634 159,-442.003 159,-431.8518\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"162.5001,-431.7912 159,-421.7913 155.5001,-431.7913 162.5001,-431.7912\"/>\n",
       "</g>\n",
       "<!-- 140080481694440 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>140080481694440</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"258.5,-281.5 185.5,-281.5 185.5,-260.5 258.5,-260.5 258.5,-281.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"222\" y=\"-267.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">TBackward</text>\n",
       "</g>\n",
       "<!-- 140080481694440&#45;&gt;140080481693936 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>140080481694440&#45;&gt;140080481693936</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M206.3565,-260.3685C189.4707,-248.8927 162.3752,-230.4783 142.8354,-217.1988\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"144.6913,-214.2284 134.4532,-211.5022 140.7567,-220.0179 144.6913,-214.2284\"/>\n",
       "</g>\n",
       "<!-- 140080481694496 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>140080481694496</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"257.5,-358 188.5,-358 188.5,-324 257.5,-324 257.5,-358\"/>\n",
       "<text text-anchor=\"middle\" x=\"223\" y=\"-344.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc2.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"223\" y=\"-331.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (256, 512)</text>\n",
       "</g>\n",
       "<!-- 140080481694496&#45;&gt;140080481694440 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>140080481694496&#45;&gt;140080481694440</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M222.7528,-323.6966C222.6152,-314.0634 222.4429,-302.003 222.2979,-291.8518\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"225.7967,-291.7402 222.1542,-281.7913 218.7975,-291.8403 225.7967,-291.7402\"/>\n",
       "</g>\n",
       "<!-- 140080481695448 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>140080481695448</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"259.5,-141.5 186.5,-141.5 186.5,-120.5 259.5,-120.5 259.5,-141.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"223\" y=\"-127.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">TBackward</text>\n",
       "</g>\n",
       "<!-- 140080481695448&#45;&gt;140080481694888 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>140080481695448&#45;&gt;140080481694888</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M205.5275,-120.2281C189.1519,-110.1325 164.4682,-94.9149 145.8209,-83.4187\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"147.5636,-80.3814 137.2145,-78.1128 143.8901,-86.3401 147.5636,-80.3814\"/>\n",
       "</g>\n",
       "<!-- 140080481695056 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>140080481695056</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"256.5,-218 189.5,-218 189.5,-184 256.5,-184 256.5,-218\"/>\n",
       "<text text-anchor=\"middle\" x=\"223\" y=\"-204.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc3.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"223\" y=\"-191.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (10, 256)</text>\n",
       "</g>\n",
       "<!-- 140080481695056&#45;&gt;140080481695448 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>140080481695056&#45;&gt;140080481695448</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M223,-183.6966C223,-174.0634 223,-162.003 223,-151.8518\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"226.5001,-151.7912 223,-141.7913 219.5001,-151.7913 226.5001,-151.7912\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f670759fda0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's visualize the network architecture\n",
    "make_dot(mlp(images.to(device=DEVICE)), params=dict(mlp.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> epoch: 0, batch index: 100, train loss: 2.237775, acc: 0.277\n",
      "==>>> epoch: 0, batch index: 200, train loss: 2.136729, acc: 0.350\n",
      "==>>> epoch: 0, batch index: 300, train loss: 1.956810, acc: 0.420\n",
      "==>>> epoch: 0, batch index: 400, train loss: 1.668826, acc: 0.473\n",
      "==>>> epoch: 0, batch index: 500, train loss: 1.353518, acc: 0.516\n",
      "==>>> epoch: 0, batch index: 600, train loss: 1.098740, acc: 0.551\n",
      "==>>> epoch: 0, batch index: 700, train loss: 0.880058, acc: 0.583\n",
      "==>>> epoch: 0, batch index: 800, train loss: 0.765013, acc: 0.609\n",
      "==>>> epoch: 0, batch index: 900, train loss: 0.672770, acc: 0.632\n",
      "==>>> epoch: 0, batch index: 938, train loss: 0.691014, acc: 0.640\n",
      "==>>> epoch: 0, batch index: 100, test loss: 0.634412, acc: 0.820\n",
      "==>>> epoch: 0, batch index: 157, test loss: 0.601172, acc: 0.825\n",
      "==>>> epoch: 1, batch index: 100, train loss: 0.602201, acc: 0.832\n",
      "==>>> epoch: 1, batch index: 200, train loss: 0.543346, acc: 0.837\n",
      "==>>> epoch: 1, batch index: 300, train loss: 0.524357, acc: 0.844\n",
      "==>>> epoch: 1, batch index: 400, train loss: 0.489157, acc: 0.850\n",
      "==>>> epoch: 1, batch index: 500, train loss: 0.478974, acc: 0.854\n",
      "==>>> epoch: 1, batch index: 600, train loss: 0.441142, acc: 0.857\n",
      "==>>> epoch: 1, batch index: 700, train loss: 0.453685, acc: 0.859\n",
      "==>>> epoch: 1, batch index: 800, train loss: 0.394165, acc: 0.862\n",
      "==>>> epoch: 1, batch index: 900, train loss: 0.445708, acc: 0.864\n",
      "==>>> epoch: 1, batch index: 938, train loss: 0.403756, acc: 0.865\n",
      "==>>> epoch: 1, batch index: 100, test loss: 0.392824, acc: 0.891\n",
      "==>>> epoch: 1, batch index: 157, test loss: 0.406360, acc: 0.891\n",
      "==>>> epoch: 2, batch index: 100, train loss: 0.423485, acc: 0.885\n",
      "==>>> epoch: 2, batch index: 200, train loss: 0.396391, acc: 0.886\n",
      "==>>> epoch: 2, batch index: 300, train loss: 0.395530, acc: 0.886\n",
      "==>>> epoch: 2, batch index: 400, train loss: 0.406304, acc: 0.888\n",
      "==>>> epoch: 2, batch index: 500, train loss: 0.395989, acc: 0.889\n",
      "==>>> epoch: 2, batch index: 600, train loss: 0.361468, acc: 0.890\n",
      "==>>> epoch: 2, batch index: 700, train loss: 0.396612, acc: 0.890\n",
      "==>>> epoch: 2, batch index: 800, train loss: 0.334388, acc: 0.891\n",
      "==>>> epoch: 2, batch index: 900, train loss: 0.342747, acc: 0.892\n",
      "==>>> epoch: 2, batch index: 938, train loss: 0.340846, acc: 0.892\n",
      "==>>> epoch: 2, batch index: 100, test loss: 0.332179, acc: 0.896\n",
      "==>>> epoch: 2, batch index: 157, test loss: 0.299247, acc: 0.898\n",
      "==>>> epoch: 3, batch index: 100, train loss: 0.356341, acc: 0.897\n",
      "==>>> epoch: 3, batch index: 200, train loss: 0.332198, acc: 0.897\n",
      "==>>> epoch: 3, batch index: 300, train loss: 0.344251, acc: 0.898\n",
      "==>>> epoch: 3, batch index: 400, train loss: 0.345222, acc: 0.901\n",
      "==>>> epoch: 3, batch index: 500, train loss: 0.305625, acc: 0.900\n",
      "==>>> epoch: 3, batch index: 600, train loss: 0.328113, acc: 0.900\n",
      "==>>> epoch: 3, batch index: 700, train loss: 0.284391, acc: 0.901\n",
      "==>>> epoch: 3, batch index: 800, train loss: 0.322256, acc: 0.902\n",
      "==>>> epoch: 3, batch index: 900, train loss: 0.293985, acc: 0.902\n",
      "==>>> epoch: 3, batch index: 938, train loss: 0.291352, acc: 0.902\n",
      "==>>> epoch: 3, batch index: 100, test loss: 0.327003, acc: 0.911\n",
      "==>>> epoch: 3, batch index: 157, test loss: 0.282910, acc: 0.909\n",
      "==>>> epoch: 4, batch index: 100, train loss: 0.312116, acc: 0.904\n",
      "==>>> epoch: 4, batch index: 200, train loss: 0.292343, acc: 0.908\n",
      "==>>> epoch: 4, batch index: 300, train loss: 0.317590, acc: 0.908\n",
      "==>>> epoch: 4, batch index: 400, train loss: 0.299278, acc: 0.908\n",
      "==>>> epoch: 4, batch index: 500, train loss: 0.298769, acc: 0.908\n",
      "==>>> epoch: 4, batch index: 600, train loss: 0.332521, acc: 0.909\n",
      "==>>> epoch: 4, batch index: 700, train loss: 0.319548, acc: 0.909\n",
      "==>>> epoch: 4, batch index: 800, train loss: 0.296625, acc: 0.909\n",
      "==>>> epoch: 4, batch index: 900, train loss: 0.302044, acc: 0.909\n",
      "==>>> epoch: 4, batch index: 938, train loss: 0.322206, acc: 0.909\n",
      "==>>> epoch: 4, batch index: 100, test loss: 0.312457, acc: 0.911\n",
      "==>>> epoch: 4, batch index: 157, test loss: 0.297976, acc: 0.914\n",
      "==>>> epoch: 5, batch index: 100, train loss: 0.292051, acc: 0.908\n",
      "==>>> epoch: 5, batch index: 200, train loss: 0.286433, acc: 0.909\n",
      "==>>> epoch: 5, batch index: 300, train loss: 0.338251, acc: 0.910\n",
      "==>>> epoch: 5, batch index: 400, train loss: 0.287029, acc: 0.911\n",
      "==>>> epoch: 5, batch index: 500, train loss: 0.276405, acc: 0.912\n",
      "==>>> epoch: 5, batch index: 600, train loss: 0.242440, acc: 0.914\n",
      "==>>> epoch: 5, batch index: 700, train loss: 0.277160, acc: 0.915\n",
      "==>>> epoch: 5, batch index: 800, train loss: 0.291216, acc: 0.915\n",
      "==>>> epoch: 5, batch index: 900, train loss: 0.253437, acc: 0.915\n",
      "==>>> epoch: 5, batch index: 938, train loss: 0.308291, acc: 0.915\n",
      "==>>> epoch: 5, batch index: 100, test loss: 0.269343, acc: 0.918\n",
      "==>>> epoch: 5, batch index: 157, test loss: 0.269460, acc: 0.920\n",
      "==>>> epoch: 6, batch index: 100, train loss: 0.285224, acc: 0.917\n",
      "==>>> epoch: 6, batch index: 200, train loss: 0.268636, acc: 0.918\n",
      "==>>> epoch: 6, batch index: 300, train loss: 0.278515, acc: 0.921\n",
      "==>>> epoch: 6, batch index: 400, train loss: 0.297025, acc: 0.921\n",
      "==>>> epoch: 6, batch index: 500, train loss: 0.294914, acc: 0.920\n",
      "==>>> epoch: 6, batch index: 600, train loss: 0.249638, acc: 0.920\n",
      "==>>> epoch: 6, batch index: 700, train loss: 0.276300, acc: 0.920\n",
      "==>>> epoch: 6, batch index: 800, train loss: 0.279447, acc: 0.919\n",
      "==>>> epoch: 6, batch index: 900, train loss: 0.242430, acc: 0.920\n",
      "==>>> epoch: 6, batch index: 938, train loss: 0.252330, acc: 0.920\n",
      "==>>> epoch: 6, batch index: 100, test loss: 0.284290, acc: 0.924\n",
      "==>>> epoch: 6, batch index: 157, test loss: 0.245295, acc: 0.925\n",
      "==>>> epoch: 7, batch index: 100, train loss: 0.269164, acc: 0.920\n",
      "==>>> epoch: 7, batch index: 200, train loss: 0.253148, acc: 0.921\n",
      "==>>> epoch: 7, batch index: 300, train loss: 0.255091, acc: 0.919\n",
      "==>>> epoch: 7, batch index: 400, train loss: 0.263019, acc: 0.921\n",
      "==>>> epoch: 7, batch index: 500, train loss: 0.259713, acc: 0.922\n",
      "==>>> epoch: 7, batch index: 600, train loss: 0.224364, acc: 0.922\n",
      "==>>> epoch: 7, batch index: 700, train loss: 0.254272, acc: 0.923\n",
      "==>>> epoch: 7, batch index: 800, train loss: 0.233885, acc: 0.924\n",
      "==>>> epoch: 7, batch index: 900, train loss: 0.203390, acc: 0.924\n",
      "==>>> epoch: 7, batch index: 938, train loss: 0.231202, acc: 0.925\n",
      "==>>> epoch: 7, batch index: 100, test loss: 0.244795, acc: 0.930\n",
      "==>>> epoch: 7, batch index: 157, test loss: 0.248787, acc: 0.926\n",
      "==>>> epoch: 8, batch index: 100, train loss: 0.273249, acc: 0.926\n",
      "==>>> epoch: 8, batch index: 200, train loss: 0.253515, acc: 0.928\n",
      "==>>> epoch: 8, batch index: 300, train loss: 0.238697, acc: 0.929\n",
      "==>>> epoch: 8, batch index: 400, train loss: 0.235866, acc: 0.928\n",
      "==>>> epoch: 8, batch index: 500, train loss: 0.200561, acc: 0.929\n",
      "==>>> epoch: 8, batch index: 600, train loss: 0.276293, acc: 0.929\n",
      "==>>> epoch: 8, batch index: 700, train loss: 0.243676, acc: 0.930\n",
      "==>>> epoch: 8, batch index: 800, train loss: 0.229580, acc: 0.930\n",
      "==>>> epoch: 8, batch index: 900, train loss: 0.213485, acc: 0.930\n",
      "==>>> epoch: 8, batch index: 938, train loss: 0.278387, acc: 0.929\n",
      "==>>> epoch: 8, batch index: 100, test loss: 0.237903, acc: 0.933\n",
      "==>>> epoch: 8, batch index: 157, test loss: 0.215827, acc: 0.934\n",
      "==>>> epoch: 9, batch index: 100, train loss: 0.260610, acc: 0.928\n",
      "==>>> epoch: 9, batch index: 200, train loss: 0.221734, acc: 0.930\n",
      "==>>> epoch: 9, batch index: 300, train loss: 0.228689, acc: 0.933\n",
      "==>>> epoch: 9, batch index: 400, train loss: 0.233655, acc: 0.932\n",
      "==>>> epoch: 9, batch index: 500, train loss: 0.191737, acc: 0.933\n",
      "==>>> epoch: 9, batch index: 600, train loss: 0.234392, acc: 0.933\n",
      "==>>> epoch: 9, batch index: 700, train loss: 0.221292, acc: 0.933\n",
      "==>>> epoch: 9, batch index: 800, train loss: 0.207946, acc: 0.933\n",
      "==>>> epoch: 9, batch index: 900, train loss: 0.238955, acc: 0.933\n",
      "==>>> epoch: 9, batch index: 938, train loss: 0.217435, acc: 0.933\n",
      "==>>> epoch: 9, batch index: 100, test loss: 0.193754, acc: 0.937\n",
      "==>>> epoch: 9, batch index: 157, test loss: 0.254654, acc: 0.936\n",
      "==>>> epoch: 10, batch index: 100, train loss: 0.221763, acc: 0.931\n",
      "==>>> epoch: 10, batch index: 200, train loss: 0.193998, acc: 0.935\n",
      "==>>> epoch: 10, batch index: 300, train loss: 0.195399, acc: 0.936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> epoch: 10, batch index: 400, train loss: 0.224081, acc: 0.935\n",
      "==>>> epoch: 10, batch index: 500, train loss: 0.237758, acc: 0.936\n",
      "==>>> epoch: 10, batch index: 600, train loss: 0.217081, acc: 0.936\n",
      "==>>> epoch: 10, batch index: 700, train loss: 0.223047, acc: 0.936\n",
      "==>>> epoch: 10, batch index: 800, train loss: 0.214435, acc: 0.936\n",
      "==>>> epoch: 10, batch index: 900, train loss: 0.235848, acc: 0.936\n",
      "==>>> epoch: 10, batch index: 938, train loss: 0.215291, acc: 0.937\n",
      "==>>> epoch: 10, batch index: 100, test loss: 0.230074, acc: 0.937\n",
      "==>>> epoch: 10, batch index: 157, test loss: 0.196517, acc: 0.939\n",
      "==>>> epoch: 11, batch index: 100, train loss: 0.208504, acc: 0.941\n",
      "==>>> epoch: 11, batch index: 200, train loss: 0.209451, acc: 0.942\n",
      "==>>> epoch: 11, batch index: 300, train loss: 0.180619, acc: 0.942\n",
      "==>>> epoch: 11, batch index: 400, train loss: 0.190278, acc: 0.942\n",
      "==>>> epoch: 11, batch index: 500, train loss: 0.219291, acc: 0.942\n",
      "==>>> epoch: 11, batch index: 600, train loss: 0.183650, acc: 0.941\n",
      "==>>> epoch: 11, batch index: 700, train loss: 0.213711, acc: 0.941\n",
      "==>>> epoch: 11, batch index: 800, train loss: 0.183192, acc: 0.941\n",
      "==>>> epoch: 11, batch index: 900, train loss: 0.192182, acc: 0.941\n",
      "==>>> epoch: 11, batch index: 938, train loss: 0.199756, acc: 0.941\n",
      "==>>> epoch: 11, batch index: 100, test loss: 0.208636, acc: 0.942\n",
      "==>>> epoch: 11, batch index: 157, test loss: 0.216624, acc: 0.940\n",
      "==>>> epoch: 12, batch index: 100, train loss: 0.172776, acc: 0.946\n",
      "==>>> epoch: 12, batch index: 200, train loss: 0.217933, acc: 0.943\n",
      "==>>> epoch: 12, batch index: 300, train loss: 0.213066, acc: 0.942\n",
      "==>>> epoch: 12, batch index: 400, train loss: 0.221516, acc: 0.942\n",
      "==>>> epoch: 12, batch index: 500, train loss: 0.196433, acc: 0.943\n",
      "==>>> epoch: 12, batch index: 600, train loss: 0.232215, acc: 0.943\n",
      "==>>> epoch: 12, batch index: 700, train loss: 0.229908, acc: 0.943\n",
      "==>>> epoch: 12, batch index: 800, train loss: 0.227670, acc: 0.943\n",
      "==>>> epoch: 12, batch index: 900, train loss: 0.196707, acc: 0.944\n",
      "==>>> epoch: 12, batch index: 938, train loss: 0.208642, acc: 0.943\n",
      "==>>> epoch: 12, batch index: 100, test loss: 0.183289, acc: 0.941\n",
      "==>>> epoch: 12, batch index: 157, test loss: 0.193540, acc: 0.943\n",
      "==>>> epoch: 13, batch index: 100, train loss: 0.181808, acc: 0.949\n",
      "==>>> epoch: 13, batch index: 200, train loss: 0.204638, acc: 0.947\n",
      "==>>> epoch: 13, batch index: 300, train loss: 0.172638, acc: 0.946\n",
      "==>>> epoch: 13, batch index: 400, train loss: 0.200591, acc: 0.946\n",
      "==>>> epoch: 13, batch index: 500, train loss: 0.194430, acc: 0.946\n",
      "==>>> epoch: 13, batch index: 600, train loss: 0.175009, acc: 0.946\n",
      "==>>> epoch: 13, batch index: 700, train loss: 0.180756, acc: 0.946\n",
      "==>>> epoch: 13, batch index: 800, train loss: 0.196244, acc: 0.946\n",
      "==>>> epoch: 13, batch index: 900, train loss: 0.184989, acc: 0.946\n",
      "==>>> epoch: 13, batch index: 938, train loss: 0.164282, acc: 0.946\n",
      "==>>> epoch: 13, batch index: 100, test loss: 0.173578, acc: 0.947\n",
      "==>>> epoch: 13, batch index: 157, test loss: 0.217139, acc: 0.946\n",
      "==>>> epoch: 14, batch index: 100, train loss: 0.168890, acc: 0.950\n",
      "==>>> epoch: 14, batch index: 200, train loss: 0.171303, acc: 0.950\n",
      "==>>> epoch: 14, batch index: 300, train loss: 0.190916, acc: 0.949\n",
      "==>>> epoch: 14, batch index: 400, train loss: 0.199620, acc: 0.949\n",
      "==>>> epoch: 14, batch index: 500, train loss: 0.177734, acc: 0.949\n",
      "==>>> epoch: 14, batch index: 600, train loss: 0.139295, acc: 0.948\n",
      "==>>> epoch: 14, batch index: 700, train loss: 0.200542, acc: 0.948\n",
      "==>>> epoch: 14, batch index: 800, train loss: 0.151066, acc: 0.949\n",
      "==>>> epoch: 14, batch index: 900, train loss: 0.154495, acc: 0.949\n",
      "==>>> epoch: 14, batch index: 938, train loss: 0.180019, acc: 0.949\n",
      "==>>> epoch: 14, batch index: 100, test loss: 0.181970, acc: 0.950\n",
      "==>>> epoch: 14, batch index: 157, test loss: 0.183720, acc: 0.949\n",
      "==>>> epoch: 15, batch index: 100, train loss: 0.171497, acc: 0.950\n",
      "==>>> epoch: 15, batch index: 200, train loss: 0.165528, acc: 0.951\n",
      "==>>> epoch: 15, batch index: 300, train loss: 0.179928, acc: 0.950\n",
      "==>>> epoch: 15, batch index: 400, train loss: 0.147768, acc: 0.951\n",
      "==>>> epoch: 15, batch index: 500, train loss: 0.201452, acc: 0.951\n",
      "==>>> epoch: 15, batch index: 600, train loss: 0.179205, acc: 0.951\n",
      "==>>> epoch: 15, batch index: 700, train loss: 0.194294, acc: 0.951\n",
      "==>>> epoch: 15, batch index: 800, train loss: 0.163202, acc: 0.951\n",
      "==>>> epoch: 15, batch index: 900, train loss: 0.137644, acc: 0.952\n",
      "==>>> epoch: 15, batch index: 938, train loss: 0.157138, acc: 0.951\n",
      "==>>> epoch: 15, batch index: 100, test loss: 0.137142, acc: 0.955\n",
      "==>>> epoch: 15, batch index: 157, test loss: 0.158047, acc: 0.952\n",
      "==>>> epoch: 16, batch index: 100, train loss: 0.149223, acc: 0.955\n",
      "==>>> epoch: 16, batch index: 200, train loss: 0.149464, acc: 0.955\n",
      "==>>> epoch: 16, batch index: 300, train loss: 0.181912, acc: 0.953\n",
      "==>>> epoch: 16, batch index: 400, train loss: 0.166781, acc: 0.955\n",
      "==>>> epoch: 16, batch index: 500, train loss: 0.160240, acc: 0.954\n",
      "==>>> epoch: 16, batch index: 600, train loss: 0.192709, acc: 0.953\n",
      "==>>> epoch: 16, batch index: 700, train loss: 0.133704, acc: 0.954\n",
      "==>>> epoch: 16, batch index: 800, train loss: 0.145637, acc: 0.954\n",
      "==>>> epoch: 16, batch index: 900, train loss: 0.149526, acc: 0.954\n",
      "==>>> epoch: 16, batch index: 938, train loss: 0.172648, acc: 0.954\n",
      "==>>> epoch: 16, batch index: 100, test loss: 0.174294, acc: 0.948\n",
      "==>>> epoch: 16, batch index: 157, test loss: 0.165842, acc: 0.951\n",
      "==>>> epoch: 17, batch index: 100, train loss: 0.177604, acc: 0.950\n",
      "==>>> epoch: 17, batch index: 200, train loss: 0.143024, acc: 0.952\n",
      "==>>> epoch: 17, batch index: 300, train loss: 0.173719, acc: 0.952\n",
      "==>>> epoch: 17, batch index: 400, train loss: 0.147164, acc: 0.954\n",
      "==>>> epoch: 17, batch index: 500, train loss: 0.164595, acc: 0.955\n",
      "==>>> epoch: 17, batch index: 600, train loss: 0.152690, acc: 0.955\n",
      "==>>> epoch: 17, batch index: 700, train loss: 0.172877, acc: 0.955\n",
      "==>>> epoch: 17, batch index: 800, train loss: 0.143409, acc: 0.955\n",
      "==>>> epoch: 17, batch index: 900, train loss: 0.136576, acc: 0.955\n",
      "==>>> epoch: 17, batch index: 938, train loss: 0.152094, acc: 0.956\n",
      "==>>> epoch: 17, batch index: 100, test loss: 0.131412, acc: 0.955\n",
      "==>>> epoch: 17, batch index: 157, test loss: 0.130398, acc: 0.955\n",
      "==>>> epoch: 18, batch index: 100, train loss: 0.148618, acc: 0.957\n",
      "==>>> epoch: 18, batch index: 200, train loss: 0.147386, acc: 0.958\n",
      "==>>> epoch: 18, batch index: 300, train loss: 0.137858, acc: 0.959\n",
      "==>>> epoch: 18, batch index: 400, train loss: 0.173065, acc: 0.957\n",
      "==>>> epoch: 18, batch index: 500, train loss: 0.142122, acc: 0.957\n",
      "==>>> epoch: 18, batch index: 600, train loss: 0.147011, acc: 0.957\n",
      "==>>> epoch: 18, batch index: 700, train loss: 0.140543, acc: 0.957\n",
      "==>>> epoch: 18, batch index: 800, train loss: 0.140316, acc: 0.957\n",
      "==>>> epoch: 18, batch index: 900, train loss: 0.130043, acc: 0.958\n",
      "==>>> epoch: 18, batch index: 938, train loss: 0.156779, acc: 0.958\n",
      "==>>> epoch: 18, batch index: 100, test loss: 0.139722, acc: 0.953\n",
      "==>>> epoch: 18, batch index: 157, test loss: 0.208565, acc: 0.956\n",
      "==>>> epoch: 19, batch index: 100, train loss: 0.118343, acc: 0.958\n",
      "==>>> epoch: 19, batch index: 200, train loss: 0.160300, acc: 0.960\n",
      "==>>> epoch: 19, batch index: 300, train loss: 0.141380, acc: 0.959\n",
      "==>>> epoch: 19, batch index: 400, train loss: 0.142202, acc: 0.960\n",
      "==>>> epoch: 19, batch index: 500, train loss: 0.110277, acc: 0.960\n",
      "==>>> epoch: 19, batch index: 600, train loss: 0.135764, acc: 0.960\n",
      "==>>> epoch: 19, batch index: 700, train loss: 0.141750, acc: 0.960\n",
      "==>>> epoch: 19, batch index: 800, train loss: 0.155826, acc: 0.960\n",
      "==>>> epoch: 19, batch index: 900, train loss: 0.136205, acc: 0.960\n",
      "==>>> epoch: 19, batch index: 938, train loss: 0.149858, acc: 0.960\n",
      "==>>> epoch: 19, batch index: 100, test loss: 0.146517, acc: 0.954\n",
      "==>>> epoch: 19, batch index: 157, test loss: 0.122330, acc: 0.958\n"
     ]
    }
   ],
   "source": [
    "# now let's train!\n",
    "\n",
    "for epoch in range(20):\n",
    "    correct_cnt, ave_loss = 0, 0\n",
    "    total_cnt = 0\n",
    "    mlp.train()\n",
    "    for batch_idx, (x, target) in enumerate(training_loader):\n",
    "        optimizer.zero_grad()\n",
    "        out = mlp(x.to(device=DEVICE))\n",
    "        loss = criterion(out, target.to(device=DEVICE))\n",
    "        _, pred_label = torch.max(out.data, 1)\n",
    "        total_cnt += x.shape[0]\n",
    "        correct_cnt += (pred_label == target.to(device=DEVICE)).sum().item()\n",
    "        ave_loss = ave_loss * 0.9 + loss.item() * 0.1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (batch_idx + 1) % 100 == 0 or (batch_idx + 1) == len(training_loader):\n",
    "            print('==>>> epoch: {}, batch index: {}, train loss: {:.6f}, acc: {:.3f}'.format(\n",
    "                epoch, batch_idx + 1, ave_loss, correct_cnt * 1.0 / total_cnt))\n",
    "\n",
    "    # testing\n",
    "    correct_cnt, ave_loss = 0, 0\n",
    "    total_cnt = 0\n",
    "    mlp.eval()\n",
    "    for batch_idx, (x, target) in enumerate(testing_loader):\n",
    "        out = mlp(x.to(device=DEVICE))\n",
    "        loss = criterion(out, target.to(device=DEVICE))\n",
    "        _, pred_label = torch.max(out.data, 1)\n",
    "        total_cnt += x.shape[0]\n",
    "        #         print(target.data)\n",
    "        correct_cnt += (pred_label == target.to(device=DEVICE)).sum().item()\n",
    "        # smooth average\n",
    "        ave_loss = ave_loss * 0.9 + loss.item() * 0.1\n",
    "\n",
    "        if (batch_idx + 1) % 100 == 0 or (batch_idx + 1) == len(testing_loader):\n",
    "            print('==>>> epoch: {}, batch index: {}, test loss: {:.6f}, acc: {:.3f}'.format(\n",
    "                epoch, batch_idx + 1, ave_loss, correct_cnt * 1.0 / total_cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4 * 4 * 50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4 * 4 * 50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "convnet = ConvNet()\n",
    "convnet.to(device=DEVICE)\n",
    "optimizer = optim.SGD(convnet.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"425pt\" height=\"734pt\"\n",
       " viewBox=\"0.00 0.00 425.00 734.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 730)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-730 421,-730 421,4 -4,4\"/>\n",
       "<!-- 140080479741824 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>140080479741824</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"314,-21 189,-21 189,0 314,0 314,-21\"/>\n",
       "<text text-anchor=\"middle\" x=\"251.5\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">LogSoftmaxBackward</text>\n",
       "</g>\n",
       "<!-- 140080479740816 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>140080479740816</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"303.5,-78 199.5,-78 199.5,-57 303.5,-57 303.5,-78\"/>\n",
       "<text text-anchor=\"middle\" x=\"251.5\" y=\"-64.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddmmBackward</text>\n",
       "</g>\n",
       "<!-- 140080479740816&#45;&gt;140080479741824 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>140080479740816&#45;&gt;140080479741824</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M251.5,-56.7787C251.5,-49.6134 251.5,-39.9517 251.5,-31.3097\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"255.0001,-31.1732 251.5,-21.1732 248.0001,-31.1732 255.0001,-31.1732\"/>\n",
       "</g>\n",
       "<!-- 140080479743224 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>140080479743224</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"186.5,-148 132.5,-148 132.5,-114 186.5,-114 186.5,-148\"/>\n",
       "<text text-anchor=\"middle\" x=\"159.5\" y=\"-134.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc2.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"159.5\" y=\"-121.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (10)</text>\n",
       "</g>\n",
       "<!-- 140080479743224&#45;&gt;140080479740816 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>140080479743224&#45;&gt;140080479740816</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M184.1543,-113.9832C197.6894,-104.641 214.3926,-93.1122 227.778,-83.8734\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"229.7865,-86.7398 236.0283,-78.1788 225.8102,-80.9788 229.7865,-86.7398\"/>\n",
       "</g>\n",
       "<!-- 140080479743840 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>140080479743840</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"298.5,-141.5 204.5,-141.5 204.5,-120.5 298.5,-120.5 298.5,-141.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"251.5\" y=\"-127.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ReluBackward0</text>\n",
       "</g>\n",
       "<!-- 140080479743840&#45;&gt;140080479740816 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>140080479743840&#45;&gt;140080479740816</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M251.5,-120.2281C251.5,-111.5091 251.5,-98.9699 251.5,-88.3068\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"255.0001,-88.1128 251.5,-78.1128 248.0001,-88.1129 255.0001,-88.1128\"/>\n",
       "</g>\n",
       "<!-- 140080479740256 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>140080479740256</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"302.5,-211.5 198.5,-211.5 198.5,-190.5 302.5,-190.5 302.5,-211.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"250.5\" y=\"-197.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddmmBackward</text>\n",
       "</g>\n",
       "<!-- 140080479740256&#45;&gt;140080479743840 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>140080479740256&#45;&gt;140080479743840</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M250.6519,-190.3685C250.7972,-180.1925 251.0206,-164.5606 251.2016,-151.8912\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"254.7034,-151.7806 251.3467,-141.7315 247.7041,-151.6805 254.7034,-151.7806\"/>\n",
       "</g>\n",
       "<!-- 140080479742216 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>140080479742216</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"186.5,-288 132.5,-288 132.5,-254 186.5,-254 186.5,-288\"/>\n",
       "<text text-anchor=\"middle\" x=\"159.5\" y=\"-274.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc1.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"159.5\" y=\"-261.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (500)</text>\n",
       "</g>\n",
       "<!-- 140080479742216&#45;&gt;140080479740256 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>140080479742216&#45;&gt;140080479740256</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M181.9944,-253.6966C196.2034,-242.7666 214.4745,-228.7119 228.5735,-217.8666\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"230.947,-220.4565 236.7393,-211.5852 226.679,-214.9081 230.947,-220.4565\"/>\n",
       "</g>\n",
       "<!-- 140080479739976 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>140080479739976</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"296.5,-281.5 204.5,-281.5 204.5,-260.5 296.5,-260.5 296.5,-281.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"250.5\" y=\"-267.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ViewBackward</text>\n",
       "</g>\n",
       "<!-- 140080479739976&#45;&gt;140080479740256 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>140080479739976&#45;&gt;140080479740256</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M250.5,-260.3685C250.5,-250.1925 250.5,-234.5606 250.5,-221.8912\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"254.0001,-221.7315 250.5,-211.7315 247.0001,-221.7316 254.0001,-221.7315\"/>\n",
       "</g>\n",
       "<!-- 140080479740032 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>140080479740032</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"330,-351.5 149,-351.5 149,-330.5 330,-330.5 330,-351.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"239.5\" y=\"-337.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MaxPool2DWithIndicesBackward</text>\n",
       "</g>\n",
       "<!-- 140080479740032&#45;&gt;140080479739976 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>140080479740032&#45;&gt;140080479739976</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M241.1707,-330.3685C242.7697,-320.1925 245.2262,-304.5606 247.2171,-291.8912\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"250.7187,-292.1537 248.8136,-281.7315 243.8036,-291.067 250.7187,-292.1537\"/>\n",
       "</g>\n",
       "<!-- 140080479740984 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>140080479740984</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"286.5,-415 192.5,-415 192.5,-394 286.5,-394 286.5,-415\"/>\n",
       "<text text-anchor=\"middle\" x=\"239.5\" y=\"-401.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ReluBackward0</text>\n",
       "</g>\n",
       "<!-- 140080479740984&#45;&gt;140080479740032 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>140080479740984&#45;&gt;140080479740032</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M239.5,-393.7281C239.5,-385.0091 239.5,-372.4699 239.5,-361.8068\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"243.0001,-361.6128 239.5,-351.6128 236.0001,-361.6129 243.0001,-361.6128\"/>\n",
       "</g>\n",
       "<!-- 140080479741768 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>140080479741768</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"318,-472 161,-472 161,-451 318,-451 318,-472\"/>\n",
       "<text text-anchor=\"middle\" x=\"239.5\" y=\"-458.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">CudnnConvolutionBackward</text>\n",
       "</g>\n",
       "<!-- 140080479741768&#45;&gt;140080479740984 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>140080479741768&#45;&gt;140080479740984</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M239.5,-450.7787C239.5,-443.6134 239.5,-433.9517 239.5,-425.3097\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"243.0001,-425.1732 239.5,-415.1732 236.0001,-425.1732 243.0001,-425.1732\"/>\n",
       "</g>\n",
       "<!-- 140080479743112 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>140080479743112</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"181,-535.5 0,-535.5 0,-514.5 181,-514.5 181,-535.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"90.5\" y=\"-521.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MaxPool2DWithIndicesBackward</text>\n",
       "</g>\n",
       "<!-- 140080479743112&#45;&gt;140080479741768 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>140080479743112&#45;&gt;140080479741768</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M115.4392,-514.3715C140.0429,-503.8861 177.8449,-487.7758 205.2558,-476.094\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"206.6522,-479.3036 214.4794,-472.1631 203.9077,-472.864 206.6522,-479.3036\"/>\n",
       "</g>\n",
       "<!-- 140080479741656 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>140080479741656</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"137.5,-599 43.5,-599 43.5,-578 137.5,-578 137.5,-599\"/>\n",
       "<text text-anchor=\"middle\" x=\"90.5\" y=\"-585.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ReluBackward0</text>\n",
       "</g>\n",
       "<!-- 140080479741656&#45;&gt;140080479743112 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>140080479741656&#45;&gt;140080479743112</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M90.5,-577.7281C90.5,-569.0091 90.5,-556.4699 90.5,-545.8068\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"94.0001,-545.6128 90.5,-535.6128 87.0001,-545.6129 94.0001,-545.6128\"/>\n",
       "</g>\n",
       "<!-- 140080479740592 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>140080479740592</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"169,-656 12,-656 12,-635 169,-635 169,-656\"/>\n",
       "<text text-anchor=\"middle\" x=\"90.5\" y=\"-642.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">CudnnConvolutionBackward</text>\n",
       "</g>\n",
       "<!-- 140080479740592&#45;&gt;140080479741656 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>140080479740592&#45;&gt;140080479741656</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M90.5,-634.7787C90.5,-627.6134 90.5,-617.9517 90.5,-609.3097\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"94.0001,-609.1732 90.5,-599.1732 87.0001,-609.1732 94.0001,-609.1732\"/>\n",
       "</g>\n",
       "<!-- 140080479743504 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>140080479743504</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"85,-726 4,-726 4,-692 85,-692 85,-726\"/>\n",
       "<text text-anchor=\"middle\" x=\"44.5\" y=\"-712.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">conv1.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"44.5\" y=\"-699.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (20, 1, 5, 5)</text>\n",
       "</g>\n",
       "<!-- 140080479743504&#45;&gt;140080479740592 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>140080479743504&#45;&gt;140080479740592</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M56.8272,-691.9832C62.9107,-683.5853 70.2742,-673.4204 76.5621,-664.7404\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"79.5945,-666.5204 82.6266,-656.3687 73.9256,-662.4138 79.5945,-666.5204\"/>\n",
       "</g>\n",
       "<!-- 140080479741880 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>140080479741880</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"171.5,-726 103.5,-726 103.5,-692 171.5,-692 171.5,-726\"/>\n",
       "<text text-anchor=\"middle\" x=\"137.5\" y=\"-712.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">conv1.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"137.5\" y=\"-699.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (20)</text>\n",
       "</g>\n",
       "<!-- 140080479741880&#45;&gt;140080479740592 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>140080479741880&#45;&gt;140080479740592</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M124.9049,-691.9832C118.6237,-683.4969 111.0069,-673.2062 104.5384,-664.4668\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"107.3071,-662.3243 98.5445,-656.3687 101.6806,-666.4888 107.3071,-662.3243\"/>\n",
       "</g>\n",
       "<!-- 140080479741600 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>140080479741600</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"280,-542 199,-542 199,-508 280,-508 280,-542\"/>\n",
       "<text text-anchor=\"middle\" x=\"239.5\" y=\"-528.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">conv2.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"239.5\" y=\"-515.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (50, 20, 5, 5)</text>\n",
       "</g>\n",
       "<!-- 140080479741600&#45;&gt;140080479741768 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>140080479741600&#45;&gt;140080479741768</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M239.5,-507.9832C239.5,-500.1157 239.5,-490.6973 239.5,-482.4019\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"243.0001,-482.3686 239.5,-472.3687 236.0001,-482.3687 243.0001,-482.3686\"/>\n",
       "</g>\n",
       "<!-- 140080479743000 -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>140080479743000</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"366.5,-542 298.5,-542 298.5,-508 366.5,-508 366.5,-542\"/>\n",
       "<text text-anchor=\"middle\" x=\"332.5\" y=\"-528.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">conv2.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"332.5\" y=\"-515.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (50)</text>\n",
       "</g>\n",
       "<!-- 140080479743000&#45;&gt;140080479741768 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>140080479743000&#45;&gt;140080479741768</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M307.5777,-507.9832C293.8955,-498.641 277.0107,-487.1122 263.4799,-477.8734\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"265.372,-474.9273 255.1398,-472.1788 261.4248,-480.7082 265.372,-474.9273\"/>\n",
       "</g>\n",
       "<!-- 140080479741432 -->\n",
       "<g id=\"node18\" class=\"node\">\n",
       "<title>140080479741432</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"409,-281.5 336,-281.5 336,-260.5 409,-260.5 409,-281.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"372.5\" y=\"-267.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">TBackward</text>\n",
       "</g>\n",
       "<!-- 140080479741432&#45;&gt;140080479740256 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>140080479741432&#45;&gt;140080479740256</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M353.9708,-260.3685C333.6098,-248.6859 300.716,-229.8125 277.4901,-216.4861\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"279.2194,-213.4431 268.8038,-211.5022 275.7356,-219.5147 279.2194,-213.4431\"/>\n",
       "</g>\n",
       "<!-- 140080479740200 -->\n",
       "<g id=\"node19\" class=\"node\">\n",
       "<title>140080479740200</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"417,-358 348,-358 348,-324 417,-324 417,-358\"/>\n",
       "<text text-anchor=\"middle\" x=\"382.5\" y=\"-344.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc1.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"382.5\" y=\"-331.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (500, 800)</text>\n",
       "</g>\n",
       "<!-- 140080479740200&#45;&gt;140080479741432 -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>140080479740200&#45;&gt;140080479741432</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M380.0281,-323.6966C378.6519,-314.0634 376.929,-302.003 375.4788,-291.8518\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"378.9207,-291.1958 374.0416,-281.7913 371.9911,-292.1858 378.9207,-291.1958\"/>\n",
       "</g>\n",
       "<!-- 140080479740648 -->\n",
       "<g id=\"node20\" class=\"node\">\n",
       "<title>140080479740648</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"391,-141.5 318,-141.5 318,-120.5 391,-120.5 391,-141.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"354.5\" y=\"-127.9\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">TBackward</text>\n",
       "</g>\n",
       "<!-- 140080479740648&#45;&gt;140080479740816 -->\n",
       "<g id=\"edge19\" class=\"edge\">\n",
       "<title>140080479740648&#45;&gt;140080479740816</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M337.0275,-120.2281C320.6519,-110.1325 295.9682,-94.9149 277.3209,-83.4187\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"279.0636,-80.3814 268.7145,-78.1128 275.3901,-86.3401 279.0636,-80.3814\"/>\n",
       "</g>\n",
       "<!-- 140080479741320 -->\n",
       "<g id=\"node21\" class=\"node\">\n",
       "<title>140080479741320</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"388,-218 321,-218 321,-184 388,-184 388,-218\"/>\n",
       "<text text-anchor=\"middle\" x=\"354.5\" y=\"-204.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">fc2.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"354.5\" y=\"-191.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (10, 500)</text>\n",
       "</g>\n",
       "<!-- 140080479741320&#45;&gt;140080479740648 -->\n",
       "<g id=\"edge20\" class=\"edge\">\n",
       "<title>140080479741320&#45;&gt;140080479740648</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M354.5,-183.6966C354.5,-174.0634 354.5,-162.003 354.5,-151.8518\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"358.0001,-151.7912 354.5,-141.7913 351.0001,-151.7913 358.0001,-151.7912\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f67073c24a8>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's visualize the network architecture\n",
    "make_dot(convnet(images.to(device=DEVICE)), params=dict(convnet.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> epoch: 0, batch index: 100, train loss: 2.258740, acc: 0.181\n",
      "==>>> epoch: 0, batch index: 200, train loss: 2.172461, acc: 0.334\n",
      "==>>> epoch: 0, batch index: 300, train loss: 1.941946, acc: 0.423\n",
      "==>>> epoch: 0, batch index: 400, train loss: 1.214224, acc: 0.495\n",
      "==>>> epoch: 0, batch index: 500, train loss: 0.627786, acc: 0.560\n",
      "==>>> epoch: 0, batch index: 600, train loss: 0.497010, acc: 0.610\n",
      "==>>> epoch: 0, batch index: 700, train loss: 0.351145, acc: 0.649\n",
      "==>>> epoch: 0, batch index: 800, train loss: 0.303818, acc: 0.681\n",
      "==>>> epoch: 0, batch index: 900, train loss: 0.336966, acc: 0.706\n",
      "==>>> epoch: 0, batch index: 938, train loss: 0.315337, acc: 0.714\n",
      "==>>> epoch: 0, batch index: 100, test loss: 0.257275, acc: 0.911\n",
      "==>>> epoch: 0, batch index: 157, test loss: 0.285459, acc: 0.915\n",
      "==>>> epoch: 1, batch index: 100, train loss: 0.271393, acc: 0.914\n",
      "==>>> epoch: 1, batch index: 200, train loss: 0.250423, acc: 0.919\n",
      "==>>> epoch: 1, batch index: 300, train loss: 0.233131, acc: 0.923\n",
      "==>>> epoch: 1, batch index: 400, train loss: 0.186542, acc: 0.925\n",
      "==>>> epoch: 1, batch index: 500, train loss: 0.226199, acc: 0.927\n",
      "==>>> epoch: 1, batch index: 600, train loss: 0.191002, acc: 0.929\n",
      "==>>> epoch: 1, batch index: 700, train loss: 0.202041, acc: 0.931\n",
      "==>>> epoch: 1, batch index: 800, train loss: 0.178331, acc: 0.933\n",
      "==>>> epoch: 1, batch index: 900, train loss: 0.153810, acc: 0.935\n",
      "==>>> epoch: 1, batch index: 938, train loss: 0.160030, acc: 0.936\n",
      "==>>> epoch: 1, batch index: 100, test loss: 0.126801, acc: 0.956\n",
      "==>>> epoch: 1, batch index: 157, test loss: 0.174512, acc: 0.955\n",
      "==>>> epoch: 2, batch index: 100, train loss: 0.171174, acc: 0.952\n",
      "==>>> epoch: 2, batch index: 200, train loss: 0.159017, acc: 0.953\n",
      "==>>> epoch: 2, batch index: 300, train loss: 0.130493, acc: 0.955\n",
      "==>>> epoch: 2, batch index: 400, train loss: 0.127255, acc: 0.956\n",
      "==>>> epoch: 2, batch index: 500, train loss: 0.144931, acc: 0.957\n",
      "==>>> epoch: 2, batch index: 600, train loss: 0.139861, acc: 0.958\n",
      "==>>> epoch: 2, batch index: 700, train loss: 0.139813, acc: 0.958\n",
      "==>>> epoch: 2, batch index: 800, train loss: 0.126601, acc: 0.959\n",
      "==>>> epoch: 2, batch index: 900, train loss: 0.132224, acc: 0.959\n",
      "==>>> epoch: 2, batch index: 938, train loss: 0.128651, acc: 0.959\n",
      "==>>> epoch: 2, batch index: 100, test loss: 0.099172, acc: 0.969\n",
      "==>>> epoch: 2, batch index: 157, test loss: 0.104825, acc: 0.968\n",
      "==>>> epoch: 3, batch index: 100, train loss: 0.119287, acc: 0.962\n",
      "==>>> epoch: 3, batch index: 200, train loss: 0.124485, acc: 0.963\n",
      "==>>> epoch: 3, batch index: 300, train loss: 0.103766, acc: 0.964\n",
      "==>>> epoch: 3, batch index: 400, train loss: 0.082888, acc: 0.966\n",
      "==>>> epoch: 3, batch index: 500, train loss: 0.100245, acc: 0.967\n",
      "==>>> epoch: 3, batch index: 600, train loss: 0.124126, acc: 0.967\n",
      "==>>> epoch: 3, batch index: 700, train loss: 0.088672, acc: 0.967\n",
      "==>>> epoch: 3, batch index: 800, train loss: 0.106786, acc: 0.967\n",
      "==>>> epoch: 3, batch index: 900, train loss: 0.090932, acc: 0.968\n",
      "==>>> epoch: 3, batch index: 938, train loss: 0.125419, acc: 0.968\n",
      "==>>> epoch: 3, batch index: 100, test loss: 0.078865, acc: 0.973\n",
      "==>>> epoch: 3, batch index: 157, test loss: 0.111405, acc: 0.974\n",
      "==>>> epoch: 4, batch index: 100, train loss: 0.085946, acc: 0.969\n",
      "==>>> epoch: 4, batch index: 200, train loss: 0.104519, acc: 0.972\n",
      "==>>> epoch: 4, batch index: 300, train loss: 0.132345, acc: 0.972\n",
      "==>>> epoch: 4, batch index: 400, train loss: 0.085515, acc: 0.972\n",
      "==>>> epoch: 4, batch index: 500, train loss: 0.082511, acc: 0.972\n",
      "==>>> epoch: 4, batch index: 600, train loss: 0.111085, acc: 0.972\n",
      "==>>> epoch: 4, batch index: 700, train loss: 0.077440, acc: 0.973\n",
      "==>>> epoch: 4, batch index: 800, train loss: 0.088484, acc: 0.973\n",
      "==>>> epoch: 4, batch index: 900, train loss: 0.064937, acc: 0.973\n",
      "==>>> epoch: 4, batch index: 938, train loss: 0.088045, acc: 0.973\n",
      "==>>> epoch: 4, batch index: 100, test loss: 0.063599, acc: 0.980\n",
      "==>>> epoch: 4, batch index: 157, test loss: 0.060787, acc: 0.980\n",
      "==>>> epoch: 5, batch index: 100, train loss: 0.080152, acc: 0.975\n",
      "==>>> epoch: 5, batch index: 200, train loss: 0.077513, acc: 0.976\n",
      "==>>> epoch: 5, batch index: 300, train loss: 0.053820, acc: 0.976\n",
      "==>>> epoch: 5, batch index: 400, train loss: 0.085409, acc: 0.976\n",
      "==>>> epoch: 5, batch index: 500, train loss: 0.084348, acc: 0.976\n",
      "==>>> epoch: 5, batch index: 600, train loss: 0.065162, acc: 0.976\n",
      "==>>> epoch: 5, batch index: 700, train loss: 0.077608, acc: 0.977\n",
      "==>>> epoch: 5, batch index: 800, train loss: 0.093156, acc: 0.977\n",
      "==>>> epoch: 5, batch index: 900, train loss: 0.120001, acc: 0.976\n",
      "==>>> epoch: 5, batch index: 938, train loss: 0.082420, acc: 0.976\n",
      "==>>> epoch: 5, batch index: 100, test loss: 0.057598, acc: 0.980\n",
      "==>>> epoch: 5, batch index: 157, test loss: 0.054254, acc: 0.981\n",
      "==>>> epoch: 6, batch index: 100, train loss: 0.063421, acc: 0.980\n",
      "==>>> epoch: 6, batch index: 200, train loss: 0.062294, acc: 0.980\n",
      "==>>> epoch: 6, batch index: 300, train loss: 0.069035, acc: 0.980\n",
      "==>>> epoch: 6, batch index: 400, train loss: 0.077058, acc: 0.979\n",
      "==>>> epoch: 6, batch index: 500, train loss: 0.080332, acc: 0.979\n",
      "==>>> epoch: 6, batch index: 600, train loss: 0.089603, acc: 0.979\n",
      "==>>> epoch: 6, batch index: 700, train loss: 0.090318, acc: 0.978\n",
      "==>>> epoch: 6, batch index: 800, train loss: 0.054153, acc: 0.979\n",
      "==>>> epoch: 6, batch index: 900, train loss: 0.049700, acc: 0.979\n",
      "==>>> epoch: 6, batch index: 938, train loss: 0.069371, acc: 0.979\n",
      "==>>> epoch: 6, batch index: 100, test loss: 0.058500, acc: 0.980\n",
      "==>>> epoch: 6, batch index: 157, test loss: 0.049798, acc: 0.981\n",
      "==>>> epoch: 7, batch index: 100, train loss: 0.064088, acc: 0.981\n",
      "==>>> epoch: 7, batch index: 200, train loss: 0.060767, acc: 0.981\n",
      "==>>> epoch: 7, batch index: 300, train loss: 0.067839, acc: 0.980\n",
      "==>>> epoch: 7, batch index: 400, train loss: 0.066629, acc: 0.980\n",
      "==>>> epoch: 7, batch index: 500, train loss: 0.039604, acc: 0.981\n",
      "==>>> epoch: 7, batch index: 600, train loss: 0.053086, acc: 0.981\n",
      "==>>> epoch: 7, batch index: 700, train loss: 0.073927, acc: 0.981\n",
      "==>>> epoch: 7, batch index: 800, train loss: 0.049757, acc: 0.981\n",
      "==>>> epoch: 7, batch index: 900, train loss: 0.046728, acc: 0.981\n",
      "==>>> epoch: 7, batch index: 938, train loss: 0.079845, acc: 0.981\n",
      "==>>> epoch: 7, batch index: 100, test loss: 0.067108, acc: 0.980\n",
      "==>>> epoch: 7, batch index: 157, test loss: 0.054016, acc: 0.980\n",
      "==>>> epoch: 8, batch index: 100, train loss: 0.054680, acc: 0.984\n",
      "==>>> epoch: 8, batch index: 200, train loss: 0.042743, acc: 0.983\n",
      "==>>> epoch: 8, batch index: 300, train loss: 0.040775, acc: 0.983\n",
      "==>>> epoch: 8, batch index: 400, train loss: 0.073961, acc: 0.982\n",
      "==>>> epoch: 8, batch index: 500, train loss: 0.057181, acc: 0.982\n",
      "==>>> epoch: 8, batch index: 600, train loss: 0.058103, acc: 0.982\n",
      "==>>> epoch: 8, batch index: 700, train loss: 0.059949, acc: 0.981\n",
      "==>>> epoch: 8, batch index: 800, train loss: 0.056015, acc: 0.981\n",
      "==>>> epoch: 8, batch index: 900, train loss: 0.060107, acc: 0.981\n",
      "==>>> epoch: 8, batch index: 938, train loss: 0.055195, acc: 0.981\n",
      "==>>> epoch: 8, batch index: 100, test loss: 0.042278, acc: 0.983\n",
      "==>>> epoch: 8, batch index: 157, test loss: 0.048191, acc: 0.983\n",
      "==>>> epoch: 9, batch index: 100, train loss: 0.051882, acc: 0.983\n",
      "==>>> epoch: 9, batch index: 200, train loss: 0.060575, acc: 0.983\n",
      "==>>> epoch: 9, batch index: 300, train loss: 0.050028, acc: 0.984\n",
      "==>>> epoch: 9, batch index: 400, train loss: 0.050772, acc: 0.984\n",
      "==>>> epoch: 9, batch index: 500, train loss: 0.062226, acc: 0.984\n",
      "==>>> epoch: 9, batch index: 600, train loss: 0.057924, acc: 0.984\n",
      "==>>> epoch: 9, batch index: 700, train loss: 0.054440, acc: 0.984\n",
      "==>>> epoch: 9, batch index: 800, train loss: 0.052882, acc: 0.984\n",
      "==>>> epoch: 9, batch index: 900, train loss: 0.050288, acc: 0.984\n",
      "==>>> epoch: 9, batch index: 938, train loss: 0.086190, acc: 0.983\n",
      "==>>> epoch: 9, batch index: 100, test loss: 0.046152, acc: 0.984\n",
      "==>>> epoch: 9, batch index: 157, test loss: 0.037244, acc: 0.984\n",
      "==>>> epoch: 10, batch index: 100, train loss: 0.040424, acc: 0.984\n",
      "==>>> epoch: 10, batch index: 200, train loss: 0.045963, acc: 0.984\n",
      "==>>> epoch: 10, batch index: 300, train loss: 0.047682, acc: 0.984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> epoch: 10, batch index: 400, train loss: 0.058421, acc: 0.984\n",
      "==>>> epoch: 10, batch index: 500, train loss: 0.050921, acc: 0.984\n",
      "==>>> epoch: 10, batch index: 600, train loss: 0.056675, acc: 0.984\n",
      "==>>> epoch: 10, batch index: 700, train loss: 0.039574, acc: 0.984\n",
      "==>>> epoch: 10, batch index: 800, train loss: 0.041488, acc: 0.984\n",
      "==>>> epoch: 10, batch index: 900, train loss: 0.038364, acc: 0.984\n",
      "==>>> epoch: 10, batch index: 938, train loss: 0.055384, acc: 0.984\n",
      "==>>> epoch: 10, batch index: 100, test loss: 0.050347, acc: 0.985\n",
      "==>>> epoch: 10, batch index: 157, test loss: 0.042828, acc: 0.986\n",
      "==>>> epoch: 11, batch index: 100, train loss: 0.043413, acc: 0.988\n",
      "==>>> epoch: 11, batch index: 200, train loss: 0.078500, acc: 0.987\n",
      "==>>> epoch: 11, batch index: 300, train loss: 0.036209, acc: 0.987\n",
      "==>>> epoch: 11, batch index: 400, train loss: 0.039824, acc: 0.987\n",
      "==>>> epoch: 11, batch index: 500, train loss: 0.046432, acc: 0.987\n",
      "==>>> epoch: 11, batch index: 600, train loss: 0.044513, acc: 0.987\n",
      "==>>> epoch: 11, batch index: 700, train loss: 0.051130, acc: 0.986\n",
      "==>>> epoch: 11, batch index: 800, train loss: 0.057048, acc: 0.986\n",
      "==>>> epoch: 11, batch index: 900, train loss: 0.045601, acc: 0.986\n",
      "==>>> epoch: 11, batch index: 938, train loss: 0.065221, acc: 0.986\n",
      "==>>> epoch: 11, batch index: 100, test loss: 0.042887, acc: 0.985\n",
      "==>>> epoch: 11, batch index: 157, test loss: 0.034455, acc: 0.986\n",
      "==>>> epoch: 12, batch index: 100, train loss: 0.056008, acc: 0.986\n",
      "==>>> epoch: 12, batch index: 200, train loss: 0.039640, acc: 0.988\n",
      "==>>> epoch: 12, batch index: 300, train loss: 0.036578, acc: 0.988\n",
      "==>>> epoch: 12, batch index: 400, train loss: 0.042194, acc: 0.987\n",
      "==>>> epoch: 12, batch index: 500, train loss: 0.038306, acc: 0.987\n",
      "==>>> epoch: 12, batch index: 600, train loss: 0.059516, acc: 0.986\n",
      "==>>> epoch: 12, batch index: 700, train loss: 0.035874, acc: 0.987\n",
      "==>>> epoch: 12, batch index: 800, train loss: 0.027182, acc: 0.987\n",
      "==>>> epoch: 12, batch index: 900, train loss: 0.050323, acc: 0.987\n",
      "==>>> epoch: 12, batch index: 938, train loss: 0.039647, acc: 0.987\n",
      "==>>> epoch: 12, batch index: 100, test loss: 0.047493, acc: 0.986\n",
      "==>>> epoch: 12, batch index: 157, test loss: 0.036316, acc: 0.986\n",
      "==>>> epoch: 13, batch index: 100, train loss: 0.038887, acc: 0.986\n",
      "==>>> epoch: 13, batch index: 200, train loss: 0.044493, acc: 0.987\n",
      "==>>> epoch: 13, batch index: 300, train loss: 0.035567, acc: 0.987\n",
      "==>>> epoch: 13, batch index: 400, train loss: 0.040750, acc: 0.987\n",
      "==>>> epoch: 13, batch index: 500, train loss: 0.038278, acc: 0.987\n",
      "==>>> epoch: 13, batch index: 600, train loss: 0.035906, acc: 0.988\n",
      "==>>> epoch: 13, batch index: 700, train loss: 0.050617, acc: 0.987\n",
      "==>>> epoch: 13, batch index: 800, train loss: 0.037654, acc: 0.987\n",
      "==>>> epoch: 13, batch index: 900, train loss: 0.045222, acc: 0.987\n",
      "==>>> epoch: 13, batch index: 938, train loss: 0.042135, acc: 0.987\n",
      "==>>> epoch: 13, batch index: 100, test loss: 0.031940, acc: 0.989\n",
      "==>>> epoch: 13, batch index: 157, test loss: 0.038861, acc: 0.988\n",
      "==>>> epoch: 14, batch index: 100, train loss: 0.036970, acc: 0.987\n",
      "==>>> epoch: 14, batch index: 200, train loss: 0.046577, acc: 0.988\n",
      "==>>> epoch: 14, batch index: 300, train loss: 0.042426, acc: 0.988\n",
      "==>>> epoch: 14, batch index: 400, train loss: 0.040679, acc: 0.988\n",
      "==>>> epoch: 14, batch index: 500, train loss: 0.036448, acc: 0.988\n",
      "==>>> epoch: 14, batch index: 600, train loss: 0.041212, acc: 0.988\n",
      "==>>> epoch: 14, batch index: 700, train loss: 0.027494, acc: 0.988\n",
      "==>>> epoch: 14, batch index: 800, train loss: 0.027227, acc: 0.988\n",
      "==>>> epoch: 14, batch index: 900, train loss: 0.042616, acc: 0.988\n",
      "==>>> epoch: 14, batch index: 938, train loss: 0.040644, acc: 0.988\n",
      "==>>> epoch: 14, batch index: 100, test loss: 0.042895, acc: 0.987\n",
      "==>>> epoch: 14, batch index: 157, test loss: 0.038359, acc: 0.987\n",
      "==>>> epoch: 15, batch index: 100, train loss: 0.046325, acc: 0.989\n",
      "==>>> epoch: 15, batch index: 200, train loss: 0.040934, acc: 0.989\n",
      "==>>> epoch: 15, batch index: 300, train loss: 0.035394, acc: 0.988\n",
      "==>>> epoch: 15, batch index: 400, train loss: 0.036398, acc: 0.988\n",
      "==>>> epoch: 15, batch index: 500, train loss: 0.042995, acc: 0.989\n",
      "==>>> epoch: 15, batch index: 600, train loss: 0.031559, acc: 0.989\n",
      "==>>> epoch: 15, batch index: 700, train loss: 0.037197, acc: 0.989\n",
      "==>>> epoch: 15, batch index: 800, train loss: 0.034126, acc: 0.988\n",
      "==>>> epoch: 15, batch index: 900, train loss: 0.039714, acc: 0.988\n",
      "==>>> epoch: 15, batch index: 938, train loss: 0.032177, acc: 0.988\n",
      "==>>> epoch: 15, batch index: 100, test loss: 0.057605, acc: 0.987\n",
      "==>>> epoch: 15, batch index: 157, test loss: 0.029496, acc: 0.988\n",
      "==>>> epoch: 16, batch index: 100, train loss: 0.043285, acc: 0.989\n",
      "==>>> epoch: 16, batch index: 200, train loss: 0.035335, acc: 0.988\n",
      "==>>> epoch: 16, batch index: 300, train loss: 0.036539, acc: 0.988\n",
      "==>>> epoch: 16, batch index: 400, train loss: 0.035582, acc: 0.989\n",
      "==>>> epoch: 16, batch index: 500, train loss: 0.035063, acc: 0.989\n",
      "==>>> epoch: 16, batch index: 600, train loss: 0.033551, acc: 0.989\n",
      "==>>> epoch: 16, batch index: 700, train loss: 0.037876, acc: 0.989\n",
      "==>>> epoch: 16, batch index: 800, train loss: 0.050238, acc: 0.989\n",
      "==>>> epoch: 16, batch index: 900, train loss: 0.048245, acc: 0.989\n",
      "==>>> epoch: 16, batch index: 938, train loss: 0.027740, acc: 0.989\n",
      "==>>> epoch: 16, batch index: 100, test loss: 0.044214, acc: 0.987\n",
      "==>>> epoch: 16, batch index: 157, test loss: 0.047219, acc: 0.988\n",
      "==>>> epoch: 17, batch index: 100, train loss: 0.029431, acc: 0.989\n",
      "==>>> epoch: 17, batch index: 200, train loss: 0.030789, acc: 0.989\n",
      "==>>> epoch: 17, batch index: 300, train loss: 0.038684, acc: 0.989\n",
      "==>>> epoch: 17, batch index: 400, train loss: 0.033142, acc: 0.990\n",
      "==>>> epoch: 17, batch index: 500, train loss: 0.035911, acc: 0.989\n",
      "==>>> epoch: 17, batch index: 600, train loss: 0.032731, acc: 0.990\n",
      "==>>> epoch: 17, batch index: 700, train loss: 0.046987, acc: 0.989\n",
      "==>>> epoch: 17, batch index: 800, train loss: 0.044599, acc: 0.989\n",
      "==>>> epoch: 17, batch index: 900, train loss: 0.034825, acc: 0.989\n",
      "==>>> epoch: 17, batch index: 938, train loss: 0.025735, acc: 0.990\n",
      "==>>> epoch: 17, batch index: 100, test loss: 0.026910, acc: 0.988\n",
      "==>>> epoch: 17, batch index: 157, test loss: 0.036748, acc: 0.987\n",
      "==>>> epoch: 18, batch index: 100, train loss: 0.021618, acc: 0.993\n",
      "==>>> epoch: 18, batch index: 200, train loss: 0.034079, acc: 0.992\n",
      "==>>> epoch: 18, batch index: 300, train loss: 0.030244, acc: 0.991\n",
      "==>>> epoch: 18, batch index: 400, train loss: 0.022596, acc: 0.991\n",
      "==>>> epoch: 18, batch index: 500, train loss: 0.028168, acc: 0.991\n",
      "==>>> epoch: 18, batch index: 600, train loss: 0.032425, acc: 0.990\n",
      "==>>> epoch: 18, batch index: 700, train loss: 0.030837, acc: 0.990\n",
      "==>>> epoch: 18, batch index: 800, train loss: 0.032249, acc: 0.990\n",
      "==>>> epoch: 18, batch index: 900, train loss: 0.029960, acc: 0.990\n",
      "==>>> epoch: 18, batch index: 938, train loss: 0.026459, acc: 0.990\n",
      "==>>> epoch: 18, batch index: 100, test loss: 0.027824, acc: 0.990\n",
      "==>>> epoch: 18, batch index: 157, test loss: 0.029897, acc: 0.990\n",
      "==>>> epoch: 19, batch index: 100, train loss: 0.027407, acc: 0.990\n",
      "==>>> epoch: 19, batch index: 200, train loss: 0.028415, acc: 0.990\n",
      "==>>> epoch: 19, batch index: 300, train loss: 0.030999, acc: 0.990\n",
      "==>>> epoch: 19, batch index: 400, train loss: 0.024541, acc: 0.990\n",
      "==>>> epoch: 19, batch index: 500, train loss: 0.027169, acc: 0.990\n",
      "==>>> epoch: 19, batch index: 600, train loss: 0.028038, acc: 0.990\n",
      "==>>> epoch: 19, batch index: 700, train loss: 0.040883, acc: 0.990\n",
      "==>>> epoch: 19, batch index: 800, train loss: 0.028387, acc: 0.990\n",
      "==>>> epoch: 19, batch index: 900, train loss: 0.018574, acc: 0.990\n",
      "==>>> epoch: 19, batch index: 938, train loss: 0.030224, acc: 0.990\n",
      "==>>> epoch: 19, batch index: 100, test loss: 0.035301, acc: 0.989\n",
      "==>>> epoch: 19, batch index: 157, test loss: 0.039333, acc: 0.987\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    #training\n",
    "\n",
    "    correct_cnt, ave_loss = 0, 0\n",
    "    total_cnt = 0\n",
    "    convnet.train()\n",
    "    for batch_idx, (x, target) in enumerate(training_loader):\n",
    "        x, target = x.to(DEVICE), target.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        out = convnet(x)\n",
    "        loss = criterion(out, target)\n",
    "        _, pred_label = torch.max(out.data, 1)\n",
    "        total_cnt += x.shape[0]\n",
    "        correct_cnt += (pred_label == target).sum().item()\n",
    "        ave_loss = ave_loss * 0.9 + loss.item() * 0.1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (batch_idx + 1) % 100 == 0 or (batch_idx + 1) == len(training_loader):\n",
    "            print('==>>> epoch: {}, batch index: {}, train loss: {:.6f}, acc: {:.3f}'.format(\n",
    "                epoch, batch_idx + 1, ave_loss, correct_cnt * 1.0 / total_cnt))\n",
    "\n",
    "\n",
    "    # testing\n",
    "    correct_cnt, ave_loss = 0, 0\n",
    "    total_cnt = 0\n",
    "    convnet.eval()\n",
    "    for batch_idx, (x, target) in enumerate(testing_loader):\n",
    "        x, target = x.to(DEVICE), target.to(DEVICE)\n",
    "        out = convnet(x)\n",
    "        loss = criterion(out, target)\n",
    "        _, pred_label = torch.max(out.data, 1)\n",
    "        total_cnt += x.shape[0]\n",
    "        #         print(target.data)\n",
    "        correct_cnt += (pred_label == target).sum().item()\n",
    "        # smooth average\n",
    "        ave_loss = ave_loss * 0.9 + loss.item() * 0.1\n",
    "\n",
    "        if (batch_idx + 1) % 100 == 0 or (batch_idx + 1) == len(testing_loader):\n",
    "            print('==>>> epoch: {}, batch index: {}, test loss: {:.6f}, acc: {:.3f}'.format(\n",
    "                epoch, batch_idx + 1, ave_loss, correct_cnt * 1.0 / total_cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lab 2 - Develop a CNN to classify the Fashion-MNIST dataset!\n",
    "\n",
    "Congratulations! You've made it to the end of module 2. Your task for Lab 2 is to develop an Artificial Neural Network to classify the Fashion-MNIST dataset that achieves at least 92.5% accuracy on test data. The dataset is available through torchvision. As we did before, train a classic softmax regression network and a simple Multi-layer Perceptron as baselines. You can find a list of networks that have been applied to the Fashion MNIST dataset [here](https://github.com/zalandoresearch/fashion-mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
